{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c54f16",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50d39a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40091 JSON files.\n",
      "Processing batch 1 (50 docs)...\n",
      "Processing batch 2 (50 docs)...\n",
      "Processing batch 3 (50 docs)...\n",
      "Processing batch 4 (50 docs)...\n",
      "Processing batch 5 (50 docs)...\n",
      "Processing batch 6 (50 docs)...\n",
      "Processing batch 7 (50 docs)...\n",
      "Processing batch 8 (50 docs)...\n",
      "Processing batch 9 (50 docs)...\n",
      "Processing batch 10 (50 docs)...\n",
      "Processing batch 11 (50 docs)...\n",
      "Processing batch 12 (50 docs)...\n",
      "Processing batch 13 (50 docs)...\n",
      "Processing batch 14 (50 docs)...\n",
      "Processing batch 15 (50 docs)...\n",
      "Processing batch 16 (50 docs)...\n",
      "Processing batch 17 (50 docs)...\n",
      "Processing batch 18 (50 docs)...\n",
      "Processing batch 19 (50 docs)...\n",
      "Processing batch 20 (50 docs)...\n",
      "Processing batch 21 (50 docs)...\n",
      "Processing batch 22 (50 docs)...\n",
      "Processing batch 23 (50 docs)...\n",
      "Processing batch 24 (50 docs)...\n",
      "Processing batch 25 (50 docs)...\n",
      "Processing batch 26 (50 docs)...\n",
      "Processing batch 27 (50 docs)...\n",
      "Processing batch 28 (50 docs)...\n",
      "Processing batch 29 (50 docs)...\n",
      "Processing batch 30 (50 docs)...\n",
      "Processing batch 31 (50 docs)...\n",
      "Processing batch 32 (50 docs)...\n",
      "Processing batch 33 (50 docs)...\n",
      "Processing batch 34 (50 docs)...\n",
      "Processing batch 35 (50 docs)...\n",
      "Processing batch 36 (50 docs)...\n",
      "Processing batch 37 (50 docs)...\n",
      "Processing batch 38 (50 docs)...\n",
      "Processing batch 39 (50 docs)...\n",
      "Processing batch 40 (50 docs)...\n",
      "Processing batch 41 (50 docs)...\n",
      "Processing batch 42 (50 docs)...\n",
      "Processing batch 43 (50 docs)...\n",
      "Processing batch 44 (50 docs)...\n",
      "Processing batch 45 (50 docs)...\n",
      "Processing batch 46 (50 docs)...\n",
      "Processing batch 47 (50 docs)...\n",
      "Processing batch 48 (50 docs)...\n",
      "Processing batch 49 (50 docs)...\n",
      "Processing batch 50 (50 docs)...\n",
      "Processing batch 51 (50 docs)...\n",
      "Processing batch 52 (50 docs)...\n",
      "Processing batch 53 (50 docs)...\n",
      "Processing batch 54 (50 docs)...\n",
      "Processing batch 55 (50 docs)...\n",
      "Processing batch 56 (50 docs)...\n",
      "Processing batch 57 (50 docs)...\n",
      "Processing batch 58 (50 docs)...\n",
      "Processing batch 59 (50 docs)...\n",
      "Processing batch 60 (50 docs)...\n",
      "Processing batch 61 (50 docs)...\n",
      "Processing batch 62 (50 docs)...\n",
      "Processing batch 63 (50 docs)...\n",
      "Processing batch 64 (50 docs)...\n",
      "Processing batch 65 (50 docs)...\n",
      "Processing batch 66 (50 docs)...\n",
      "Processing batch 67 (50 docs)...\n",
      "Processing batch 68 (50 docs)...\n",
      "Processing batch 69 (50 docs)...\n",
      "Processing batch 70 (50 docs)...\n",
      "Processing batch 71 (50 docs)...\n",
      "Processing batch 72 (50 docs)...\n",
      "Processing batch 73 (50 docs)...\n",
      "Processing batch 74 (50 docs)...\n",
      "Processing batch 75 (50 docs)...\n",
      "Processing batch 76 (50 docs)...\n",
      "Processing batch 77 (50 docs)...\n",
      "Processing batch 78 (50 docs)...\n",
      "Processing batch 79 (50 docs)...\n",
      "Processing batch 80 (50 docs)...\n",
      "Processing batch 81 (50 docs)...\n",
      "Processing batch 82 (50 docs)...\n",
      "Processing batch 83 (50 docs)...\n",
      "Processing batch 84 (50 docs)...\n",
      "Processing batch 85 (50 docs)...\n",
      "Processing batch 86 (50 docs)...\n",
      "Processing batch 87 (50 docs)...\n",
      "Processing batch 88 (50 docs)...\n",
      "Processing batch 89 (50 docs)...\n",
      "Processing batch 90 (50 docs)...\n",
      "Processing batch 91 (50 docs)...\n",
      "Processing batch 92 (50 docs)...\n",
      "Processing batch 93 (50 docs)...\n",
      "Processing batch 94 (50 docs)...\n",
      "Processing batch 95 (50 docs)...\n",
      "Processing batch 96 (50 docs)...\n",
      "Processing batch 97 (50 docs)...\n",
      "Processing batch 98 (50 docs)...\n",
      "Processing batch 99 (50 docs)...\n",
      "Processing batch 100 (50 docs)...\n",
      "Processing batch 101 (50 docs)...\n",
      "Processing batch 102 (50 docs)...\n",
      "Processing batch 103 (50 docs)...\n",
      "Processing batch 104 (50 docs)...\n",
      "Processing batch 105 (50 docs)...\n",
      "Processing batch 106 (50 docs)...\n",
      "Processing batch 107 (50 docs)...\n",
      "Processing batch 108 (50 docs)...\n",
      "Processing batch 109 (50 docs)...\n",
      "Processing batch 110 (50 docs)...\n",
      "Processing batch 111 (50 docs)...\n",
      "Processing batch 112 (50 docs)...\n",
      "Processing batch 113 (50 docs)...\n",
      "Processing batch 114 (50 docs)...\n",
      "Processing batch 115 (50 docs)...\n",
      "Processing batch 116 (50 docs)...\n",
      "Processing batch 117 (50 docs)...\n",
      "Processing batch 118 (50 docs)...\n",
      "Processing batch 119 (50 docs)...\n",
      "Processing batch 120 (50 docs)...\n",
      "Processing batch 121 (50 docs)...\n",
      "Processing batch 122 (50 docs)...\n",
      "Processing batch 123 (50 docs)...\n",
      "Processing batch 124 (50 docs)...\n",
      "Processing batch 125 (50 docs)...\n",
      "Processing batch 126 (50 docs)...\n",
      "Processing batch 127 (50 docs)...\n",
      "Processing batch 128 (50 docs)...\n",
      "Processing batch 129 (50 docs)...\n",
      "Processing batch 130 (50 docs)...\n",
      "Processing batch 131 (50 docs)...\n",
      "Processing batch 132 (50 docs)...\n",
      "Processing batch 133 (50 docs)...\n",
      "Processing batch 134 (50 docs)...\n",
      "Processing batch 135 (50 docs)...\n",
      "Processing batch 136 (50 docs)...\n",
      "Processing batch 137 (50 docs)...\n",
      "Processing batch 138 (50 docs)...\n",
      "Processing batch 139 (50 docs)...\n",
      "Processing batch 140 (50 docs)...\n",
      "Processing batch 141 (50 docs)...\n",
      "Processing batch 142 (50 docs)...\n",
      "Processing batch 143 (50 docs)...\n",
      "Processing batch 144 (50 docs)...\n",
      "Processing batch 145 (50 docs)...\n",
      "Processing batch 146 (50 docs)...\n",
      "Processing batch 147 (50 docs)...\n",
      "Processing batch 148 (50 docs)...\n",
      "Processing batch 149 (50 docs)...\n",
      "Processing batch 150 (50 docs)...\n",
      "Processing batch 151 (50 docs)...\n",
      "Processing batch 152 (50 docs)...\n",
      "Processing batch 153 (50 docs)...\n",
      "Processing batch 154 (50 docs)...\n",
      "Processing batch 155 (50 docs)...\n",
      "Processing batch 156 (50 docs)...\n",
      "Processing batch 157 (50 docs)...\n",
      "Processing batch 158 (50 docs)...\n",
      "Processing batch 159 (50 docs)...\n",
      "Processing batch 160 (50 docs)...\n",
      "Processing batch 161 (50 docs)...\n",
      "Processing batch 162 (50 docs)...\n",
      "Processing batch 163 (50 docs)...\n",
      "Processing batch 164 (50 docs)...\n",
      "Processing batch 165 (50 docs)...\n",
      "Processing batch 166 (50 docs)...\n",
      "Processing batch 167 (50 docs)...\n",
      "Processing batch 168 (50 docs)...\n",
      "Processing batch 169 (50 docs)...\n",
      "Processing batch 170 (50 docs)...\n",
      "Processing batch 171 (50 docs)...\n",
      "Processing batch 172 (50 docs)...\n",
      "Processing batch 173 (50 docs)...\n",
      "Processing batch 174 (50 docs)...\n",
      "Processing batch 175 (50 docs)...\n",
      "Processing batch 176 (50 docs)...\n",
      "Processing batch 177 (50 docs)...\n",
      "Processing batch 178 (50 docs)...\n",
      "Processing batch 179 (50 docs)...\n",
      "Processing batch 180 (50 docs)...\n",
      "Processing batch 181 (50 docs)...\n",
      "Processing batch 182 (50 docs)...\n",
      "Processing batch 183 (50 docs)...\n",
      "Processing batch 184 (50 docs)...\n",
      "Processing batch 185 (50 docs)...\n",
      "Processing batch 186 (50 docs)...\n",
      "Processing batch 187 (50 docs)...\n",
      "Processing batch 188 (50 docs)...\n",
      "Processing batch 189 (50 docs)...\n",
      "Processing batch 190 (50 docs)...\n",
      "Processing batch 191 (50 docs)...\n",
      "Processing batch 192 (50 docs)...\n",
      "Processing batch 193 (50 docs)...\n",
      "Processing batch 194 (50 docs)...\n",
      "Processing batch 195 (50 docs)...\n",
      "Processing batch 196 (50 docs)...\n",
      "Processing batch 197 (50 docs)...\n",
      "Processing batch 198 (50 docs)...\n",
      "Processing batch 199 (50 docs)...\n",
      "Processing batch 200 (50 docs)...\n",
      "Processing batch 201 (50 docs)...\n",
      "Processing batch 202 (50 docs)...\n",
      "Processing batch 203 (50 docs)...\n",
      "Processing batch 204 (50 docs)...\n",
      "Processing batch 205 (50 docs)...\n",
      "Processing batch 206 (50 docs)...\n",
      "Processing batch 207 (50 docs)...\n",
      "Processing batch 208 (50 docs)...\n",
      "Processing batch 209 (50 docs)...\n",
      "Processing batch 210 (50 docs)...\n",
      "Processing batch 211 (50 docs)...\n",
      "Processing batch 212 (50 docs)...\n",
      "Processing batch 213 (50 docs)...\n",
      "Processing batch 214 (50 docs)...\n",
      "Processing batch 215 (50 docs)...\n",
      "Processing batch 216 (50 docs)...\n",
      "Processing batch 217 (50 docs)...\n",
      "Processing batch 218 (50 docs)...\n",
      "Processing batch 219 (50 docs)...\n",
      "Processing batch 220 (50 docs)...\n",
      "Processing batch 221 (50 docs)...\n",
      "Processing batch 222 (50 docs)...\n",
      "Processing batch 223 (50 docs)...\n",
      "Processing batch 224 (50 docs)...\n",
      "Processing batch 225 (50 docs)...\n",
      "Processing batch 226 (50 docs)...\n",
      "Processing batch 227 (50 docs)...\n",
      "Processing batch 228 (50 docs)...\n",
      "Processing batch 229 (50 docs)...\n",
      "Processing batch 230 (50 docs)...\n",
      "Processing batch 231 (50 docs)...\n",
      "Processing batch 232 (50 docs)...\n",
      "Processing batch 233 (50 docs)...\n",
      "Processing batch 234 (50 docs)...\n",
      "Processing batch 235 (50 docs)...\n",
      "Processing batch 236 (50 docs)...\n",
      "Processing batch 237 (50 docs)...\n",
      "Processing batch 238 (50 docs)...\n",
      "Processing batch 239 (50 docs)...\n",
      "Processing batch 240 (50 docs)...\n",
      "Processing batch 241 (50 docs)...\n",
      "Processing batch 242 (50 docs)...\n",
      "Processing batch 243 (50 docs)...\n",
      "Processing batch 244 (50 docs)...\n",
      "Processing batch 245 (50 docs)...\n",
      "Processing batch 246 (50 docs)...\n",
      "Processing batch 247 (50 docs)...\n",
      "Processing batch 248 (50 docs)...\n",
      "Processing batch 249 (50 docs)...\n",
      "Processing batch 250 (50 docs)...\n",
      "Processing batch 251 (50 docs)...\n",
      "Processing batch 252 (50 docs)...\n",
      "Processing batch 253 (50 docs)...\n",
      "Processing batch 254 (50 docs)...\n",
      "Processing batch 255 (50 docs)...\n",
      "Processing batch 256 (50 docs)...\n",
      "Processing batch 257 (50 docs)...\n",
      "Processing batch 258 (50 docs)...\n",
      "Processing batch 259 (50 docs)...\n",
      "Processing batch 260 (50 docs)...\n",
      "Processing batch 261 (50 docs)...\n",
      "Processing batch 262 (50 docs)...\n",
      "Processing batch 263 (50 docs)...\n",
      "Processing batch 264 (50 docs)...\n",
      "Processing batch 265 (50 docs)...\n",
      "Processing batch 266 (50 docs)...\n",
      "Processing batch 267 (50 docs)...\n",
      "Processing batch 268 (50 docs)...\n",
      "Processing batch 269 (50 docs)...\n",
      "Processing batch 270 (50 docs)...\n",
      "Processing batch 271 (50 docs)...\n",
      "Processing batch 272 (50 docs)...\n",
      "Processing batch 273 (50 docs)...\n",
      "Processing batch 274 (50 docs)...\n",
      "Processing batch 275 (50 docs)...\n",
      "Processing batch 276 (50 docs)...\n",
      "Processing batch 277 (50 docs)...\n",
      "Processing batch 278 (50 docs)...\n",
      "Processing batch 279 (50 docs)...\n",
      "Processing batch 280 (50 docs)...\n",
      "Processing batch 281 (50 docs)...\n",
      "Processing batch 282 (50 docs)...\n",
      "Processing batch 283 (50 docs)...\n",
      "Processing batch 284 (50 docs)...\n",
      "Processing batch 285 (50 docs)...\n",
      "Processing batch 286 (50 docs)...\n",
      "Processing batch 287 (50 docs)...\n",
      "Processing batch 288 (50 docs)...\n",
      "Processing batch 289 (50 docs)...\n",
      "Processing batch 290 (50 docs)...\n",
      "Processing batch 291 (50 docs)...\n",
      "Processing batch 292 (50 docs)...\n",
      "Processing batch 293 (50 docs)...\n",
      "Processing batch 294 (50 docs)...\n",
      "Processing batch 295 (50 docs)...\n",
      "Processing batch 296 (50 docs)...\n",
      "Processing batch 297 (50 docs)...\n",
      "Processing batch 298 (50 docs)...\n",
      "Processing batch 299 (50 docs)...\n",
      "Processing batch 300 (50 docs)...\n",
      "Processing batch 301 (50 docs)...\n",
      "Processing batch 302 (50 docs)...\n",
      "Processing batch 303 (50 docs)...\n",
      "Processing batch 304 (50 docs)...\n",
      "Processing batch 305 (50 docs)...\n",
      "Processing batch 306 (50 docs)...\n",
      "Processing batch 307 (50 docs)...\n",
      "Processing batch 308 (50 docs)...\n",
      "Processing batch 309 (50 docs)...\n",
      "Processing batch 310 (50 docs)...\n",
      "Processing batch 311 (50 docs)...\n",
      "Processing batch 312 (50 docs)...\n",
      "Processing batch 313 (50 docs)...\n",
      "Processing batch 314 (50 docs)...\n",
      "Processing batch 315 (50 docs)...\n",
      "Processing batch 316 (50 docs)...\n",
      "Processing batch 317 (50 docs)...\n",
      "Processing batch 318 (50 docs)...\n",
      "Processing batch 319 (50 docs)...\n",
      "Processing batch 320 (50 docs)...\n",
      "Processing batch 321 (50 docs)...\n",
      "Processing batch 322 (50 docs)...\n",
      "Processing batch 323 (50 docs)...\n",
      "Processing batch 324 (50 docs)...\n",
      "Processing batch 325 (50 docs)...\n",
      "Processing batch 326 (50 docs)...\n",
      "Processing batch 327 (50 docs)...\n",
      "Processing batch 328 (50 docs)...\n",
      "Processing batch 329 (50 docs)...\n",
      "Processing batch 330 (50 docs)...\n",
      "Processing batch 331 (50 docs)...\n",
      "Processing batch 332 (50 docs)...\n",
      "Processing batch 333 (50 docs)...\n",
      "Processing batch 334 (50 docs)...\n",
      "Processing batch 335 (50 docs)...\n",
      "Processing batch 336 (50 docs)...\n",
      "Processing batch 337 (50 docs)...\n",
      "Processing batch 338 (50 docs)...\n",
      "Processing batch 339 (50 docs)...\n",
      "Processing batch 340 (50 docs)...\n",
      "Processing batch 341 (50 docs)...\n",
      "Processing batch 342 (50 docs)...\n",
      "Processing batch 343 (50 docs)...\n",
      "Processing batch 344 (50 docs)...\n",
      "Processing batch 345 (50 docs)...\n",
      "Processing batch 346 (50 docs)...\n",
      "Processing batch 347 (50 docs)...\n",
      "Processing batch 348 (50 docs)...\n",
      "Processing batch 349 (50 docs)...\n",
      "Processing batch 350 (50 docs)...\n",
      "Processing batch 351 (50 docs)...\n",
      "Processing batch 352 (50 docs)...\n",
      "Processing batch 353 (50 docs)...\n",
      "Processing batch 354 (50 docs)...\n",
      "Processing batch 355 (50 docs)...\n",
      "Processing batch 356 (50 docs)...\n",
      "Processing batch 357 (50 docs)...\n",
      "Processing batch 358 (50 docs)...\n",
      "Processing batch 359 (50 docs)...\n",
      "Processing batch 360 (50 docs)...\n",
      "Processing batch 361 (50 docs)...\n",
      "Processing batch 362 (50 docs)...\n",
      "Processing batch 363 (50 docs)...\n",
      "Processing batch 364 (50 docs)...\n",
      "Processing batch 365 (50 docs)...\n",
      "Processing batch 366 (50 docs)...\n",
      "Processing batch 367 (50 docs)...\n",
      "Processing batch 368 (50 docs)...\n",
      "Processing batch 369 (50 docs)...\n",
      "Processing batch 370 (50 docs)...\n",
      "Processing batch 371 (50 docs)...\n",
      "Processing batch 372 (50 docs)...\n",
      "Processing batch 373 (50 docs)...\n",
      "Processing batch 374 (50 docs)...\n",
      "Processing batch 375 (50 docs)...\n",
      "Processing batch 376 (50 docs)...\n",
      "Processing batch 377 (50 docs)...\n",
      "Processing batch 378 (50 docs)...\n",
      "Processing batch 379 (50 docs)...\n",
      "Processing batch 380 (50 docs)...\n",
      "Processing batch 381 (50 docs)...\n",
      "Processing batch 382 (50 docs)...\n",
      "Processing batch 383 (50 docs)...\n",
      "Processing batch 384 (50 docs)...\n",
      "Processing batch 385 (50 docs)...\n",
      "Processing batch 386 (50 docs)...\n",
      "Processing batch 387 (50 docs)...\n",
      "Processing batch 388 (50 docs)...\n",
      "Processing batch 389 (50 docs)...\n",
      "Processing batch 390 (50 docs)...\n",
      "Processing batch 391 (50 docs)...\n",
      "Processing batch 392 (50 docs)...\n",
      "Processing batch 393 (50 docs)...\n",
      "Processing batch 394 (50 docs)...\n",
      "Processing batch 395 (50 docs)...\n",
      "Processing batch 396 (50 docs)...\n",
      "Processing batch 397 (50 docs)...\n",
      "Processing batch 398 (50 docs)...\n",
      "Processing batch 399 (50 docs)...\n",
      "Processing batch 400 (50 docs)...\n",
      "Processing batch 401 (50 docs)...\n",
      "Processing batch 402 (50 docs)...\n",
      "Processing batch 403 (50 docs)...\n",
      "Processing batch 404 (50 docs)...\n",
      "Processing batch 405 (50 docs)...\n",
      "Processing batch 406 (50 docs)...\n",
      "Processing batch 407 (50 docs)...\n",
      "Processing batch 408 (50 docs)...\n",
      "Processing batch 409 (50 docs)...\n",
      "Processing batch 410 (50 docs)...\n",
      "Processing batch 411 (50 docs)...\n",
      "Processing batch 412 (50 docs)...\n",
      "Processing batch 413 (50 docs)...\n",
      "Processing batch 414 (50 docs)...\n",
      "Processing batch 415 (50 docs)...\n",
      "Processing batch 416 (50 docs)...\n",
      "Processing batch 417 (50 docs)...\n",
      "Processing batch 418 (50 docs)...\n",
      "Processing batch 419 (50 docs)...\n",
      "Processing batch 420 (50 docs)...\n",
      "Processing batch 421 (50 docs)...\n",
      "Processing batch 422 (50 docs)...\n",
      "Processing batch 423 (50 docs)...\n",
      "Processing batch 424 (50 docs)...\n",
      "Processing batch 425 (50 docs)...\n",
      "Processing batch 426 (50 docs)...\n",
      "Processing batch 427 (50 docs)...\n",
      "Processing batch 428 (50 docs)...\n",
      "Processing batch 429 (50 docs)...\n",
      "Processing batch 430 (50 docs)...\n",
      "Processing batch 431 (50 docs)...\n",
      "Processing batch 432 (50 docs)...\n",
      "Processing batch 433 (50 docs)...\n",
      "Processing batch 434 (50 docs)...\n",
      "Processing batch 435 (50 docs)...\n",
      "Processing batch 436 (50 docs)...\n",
      "Processing batch 437 (50 docs)...\n",
      "Processing batch 438 (50 docs)...\n",
      "Processing batch 439 (50 docs)...\n",
      "Processing batch 440 (50 docs)...\n",
      "Processing batch 441 (50 docs)...\n",
      "Processing batch 442 (50 docs)...\n",
      "Processing batch 443 (50 docs)...\n",
      "Processing batch 444 (50 docs)...\n",
      "Processing batch 445 (50 docs)...\n",
      "Processing batch 446 (50 docs)...\n",
      "Processing batch 447 (50 docs)...\n",
      "Processing batch 448 (50 docs)...\n",
      "Processing batch 449 (50 docs)...\n",
      "Processing batch 450 (50 docs)...\n",
      "Processing batch 451 (50 docs)...\n",
      "Processing batch 452 (50 docs)...\n",
      "Processing batch 453 (50 docs)...\n",
      "Processing batch 454 (50 docs)...\n",
      "Processing batch 455 (50 docs)...\n",
      "Processing batch 456 (50 docs)...\n",
      "Processing batch 457 (50 docs)...\n",
      "Processing batch 458 (50 docs)...\n",
      "Processing batch 459 (50 docs)...\n",
      "Processing batch 460 (50 docs)...\n",
      "Processing batch 461 (50 docs)...\n",
      "Processing batch 462 (50 docs)...\n",
      "Processing batch 463 (50 docs)...\n",
      "Processing batch 464 (50 docs)...\n",
      "Processing batch 465 (50 docs)...\n",
      "Processing batch 466 (50 docs)...\n",
      "Processing batch 467 (50 docs)...\n",
      "Processing batch 468 (50 docs)...\n",
      "Processing batch 469 (50 docs)...\n",
      "Processing batch 470 (50 docs)...\n",
      "Processing batch 471 (50 docs)...\n",
      "Processing batch 472 (50 docs)...\n",
      "Processing batch 473 (50 docs)...\n",
      "Processing batch 474 (50 docs)...\n",
      "Processing batch 475 (50 docs)...\n",
      "Processing batch 476 (50 docs)...\n",
      "Processing batch 477 (50 docs)...\n",
      "Processing batch 478 (50 docs)...\n",
      "Processing batch 479 (50 docs)...\n",
      "Processing batch 480 (50 docs)...\n",
      "Processing batch 481 (50 docs)...\n",
      "Processing batch 482 (50 docs)...\n",
      "Processing batch 483 (50 docs)...\n",
      "Processing batch 484 (50 docs)...\n",
      "Processing batch 485 (50 docs)...\n",
      "Processing batch 486 (50 docs)...\n",
      "Processing batch 487 (50 docs)...\n",
      "Processing batch 488 (50 docs)...\n",
      "Processing batch 489 (50 docs)...\n",
      "Processing batch 490 (50 docs)...\n",
      "Processing batch 491 (50 docs)...\n",
      "Processing batch 492 (50 docs)...\n",
      "Processing batch 493 (50 docs)...\n",
      "Processing batch 494 (50 docs)...\n",
      "Processing batch 495 (50 docs)...\n",
      "Processing batch 496 (50 docs)...\n",
      "Processing batch 497 (50 docs)...\n",
      "Processing batch 498 (50 docs)...\n",
      "Processing batch 499 (50 docs)...\n",
      "Processing batch 500 (50 docs)...\n",
      "Processing batch 501 (50 docs)...\n",
      "Processing batch 502 (50 docs)...\n",
      "Processing batch 503 (50 docs)...\n",
      "Processing batch 504 (50 docs)...\n",
      "Processing batch 505 (50 docs)...\n",
      "Processing batch 506 (50 docs)...\n",
      "Processing batch 507 (50 docs)...\n",
      "Processing batch 508 (50 docs)...\n",
      "Processing batch 509 (50 docs)...\n",
      "Processing batch 510 (50 docs)...\n",
      "Processing batch 511 (50 docs)...\n",
      "Processing batch 512 (50 docs)...\n",
      "Processing batch 513 (50 docs)...\n",
      "Processing batch 514 (50 docs)...\n",
      "Processing batch 515 (50 docs)...\n",
      "Processing batch 516 (50 docs)...\n",
      "Processing batch 517 (50 docs)...\n",
      "Processing batch 518 (50 docs)...\n",
      "Processing batch 519 (50 docs)...\n",
      "Processing batch 520 (50 docs)...\n",
      "Processing batch 521 (50 docs)...\n",
      "Processing batch 522 (50 docs)...\n",
      "Processing batch 523 (50 docs)...\n",
      "Processing batch 524 (50 docs)...\n",
      "Processing batch 525 (50 docs)...\n",
      "Processing batch 526 (50 docs)...\n",
      "Processing batch 527 (50 docs)...\n",
      "Processing batch 528 (50 docs)...\n",
      "Processing batch 529 (50 docs)...\n",
      "Processing batch 530 (50 docs)...\n",
      "Processing batch 531 (50 docs)...\n",
      "Processing batch 532 (50 docs)...\n",
      "Processing batch 533 (50 docs)...\n",
      "Processing batch 534 (50 docs)...\n",
      "Processing batch 535 (50 docs)...\n",
      "Processing batch 536 (50 docs)...\n",
      "Processing batch 537 (50 docs)...\n",
      "Processing batch 538 (50 docs)...\n",
      "Processing batch 539 (50 docs)...\n",
      "Processing batch 540 (50 docs)...\n",
      "Processing batch 541 (50 docs)...\n",
      "Processing batch 542 (50 docs)...\n",
      "Processing batch 543 (50 docs)...\n",
      "Processing batch 544 (50 docs)...\n",
      "Processing batch 545 (50 docs)...\n",
      "Processing batch 546 (50 docs)...\n",
      "Processing batch 547 (50 docs)...\n",
      "Processing batch 548 (50 docs)...\n",
      "Processing batch 549 (50 docs)...\n",
      "Processing batch 550 (50 docs)...\n",
      "Processing batch 551 (50 docs)...\n",
      "Processing batch 552 (50 docs)...\n",
      "Processing batch 553 (50 docs)...\n",
      "Processing batch 554 (50 docs)...\n",
      "Processing batch 555 (50 docs)...\n",
      "Processing batch 556 (50 docs)...\n",
      "Processing batch 557 (50 docs)...\n",
      "Processing batch 558 (50 docs)...\n",
      "Processing batch 559 (50 docs)...\n",
      "Processing batch 560 (50 docs)...\n",
      "Processing batch 561 (50 docs)...\n",
      "Processing batch 562 (50 docs)...\n",
      "Processing batch 563 (50 docs)...\n",
      "Processing batch 564 (50 docs)...\n",
      "Processing batch 565 (50 docs)...\n",
      "Processing batch 566 (50 docs)...\n",
      "Processing batch 567 (50 docs)...\n",
      "Processing batch 568 (50 docs)...\n",
      "Processing batch 569 (50 docs)...\n",
      "Processing batch 570 (50 docs)...\n",
      "Processing batch 571 (50 docs)...\n",
      "Processing batch 572 (50 docs)...\n",
      "Processing batch 573 (50 docs)...\n",
      "Processing batch 574 (50 docs)...\n",
      "Processing batch 575 (50 docs)...\n",
      "Processing batch 576 (50 docs)...\n",
      "Processing batch 577 (50 docs)...\n",
      "Processing batch 578 (50 docs)...\n",
      "Processing batch 579 (50 docs)...\n",
      "Processing batch 580 (50 docs)...\n",
      "Processing batch 581 (50 docs)...\n",
      "Processing batch 582 (50 docs)...\n",
      "Processing batch 583 (50 docs)...\n",
      "Processing batch 584 (50 docs)...\n",
      "Processing batch 585 (50 docs)...\n",
      "Processing batch 586 (50 docs)...\n",
      "Processing batch 587 (50 docs)...\n",
      "Processing batch 588 (50 docs)...\n",
      "Processing batch 589 (50 docs)...\n",
      "Processing batch 590 (50 docs)...\n",
      "Processing batch 591 (50 docs)...\n",
      "Processing batch 592 (50 docs)...\n",
      "Processing batch 593 (50 docs)...\n",
      "Processing batch 594 (50 docs)...\n",
      "Processing batch 595 (50 docs)...\n",
      "Processing batch 596 (50 docs)...\n",
      "Processing batch 597 (50 docs)...\n",
      "Processing batch 598 (50 docs)...\n",
      "Processing batch 599 (50 docs)...\n",
      "Processing batch 600 (50 docs)...\n",
      "Processing batch 601 (50 docs)...\n",
      "Processing batch 602 (50 docs)...\n",
      "Processing batch 603 (50 docs)...\n",
      "Processing batch 604 (50 docs)...\n",
      "Processing batch 605 (50 docs)...\n",
      "Processing batch 606 (50 docs)...\n",
      "Processing batch 607 (50 docs)...\n",
      "Processing batch 608 (50 docs)...\n",
      "Processing batch 609 (50 docs)...\n",
      "Processing batch 610 (50 docs)...\n",
      "Processing batch 611 (50 docs)...\n",
      "Processing batch 612 (50 docs)...\n",
      "Processing batch 613 (50 docs)...\n",
      "Processing batch 614 (50 docs)...\n",
      "Processing batch 615 (50 docs)...\n",
      "Processing batch 616 (50 docs)...\n",
      "Processing batch 617 (50 docs)...\n",
      "Processing batch 618 (50 docs)...\n",
      "Processing batch 619 (50 docs)...\n",
      "Processing batch 620 (50 docs)...\n",
      "Processing batch 621 (50 docs)...\n",
      "Processing batch 622 (50 docs)...\n",
      "Processing batch 623 (50 docs)...\n",
      "Processing batch 624 (50 docs)...\n",
      "Processing batch 625 (50 docs)...\n",
      "Processing batch 626 (50 docs)...\n",
      "Processing batch 627 (50 docs)...\n",
      "Processing batch 628 (50 docs)...\n",
      "Processing batch 629 (50 docs)...\n",
      "Processing batch 630 (50 docs)...\n",
      "Processing batch 631 (50 docs)...\n",
      "Processing batch 632 (50 docs)...\n",
      "Processing batch 633 (50 docs)...\n",
      "Processing batch 634 (50 docs)...\n",
      "Processing batch 635 (50 docs)...\n",
      "Processing batch 636 (50 docs)...\n",
      "Processing batch 637 (50 docs)...\n",
      "Processing batch 638 (50 docs)...\n",
      "Processing batch 639 (50 docs)...\n",
      "Processing batch 640 (50 docs)...\n",
      "Processing batch 641 (50 docs)...\n",
      "Processing batch 642 (50 docs)...\n",
      "Processing batch 643 (50 docs)...\n",
      "Processing batch 644 (50 docs)...\n",
      "Processing batch 645 (50 docs)...\n",
      "Processing batch 646 (50 docs)...\n",
      "Processing batch 647 (50 docs)...\n",
      "Processing batch 648 (50 docs)...\n",
      "Processing batch 649 (50 docs)...\n",
      "Processing batch 650 (50 docs)...\n",
      "Processing batch 651 (50 docs)...\n",
      "Processing batch 652 (50 docs)...\n",
      "Processing batch 653 (50 docs)...\n",
      "Processing batch 654 (50 docs)...\n",
      "Processing batch 655 (50 docs)...\n",
      "Processing batch 656 (50 docs)...\n",
      "Processing batch 657 (50 docs)...\n",
      "Processing batch 658 (50 docs)...\n",
      "Processing batch 659 (50 docs)...\n",
      "Processing batch 660 (50 docs)...\n",
      "Processing batch 661 (50 docs)...\n",
      "Processing batch 662 (50 docs)...\n",
      "Processing batch 663 (50 docs)...\n",
      "Processing batch 664 (50 docs)...\n",
      "Processing batch 665 (50 docs)...\n",
      "Processing batch 666 (50 docs)...\n",
      "Processing batch 667 (50 docs)...\n",
      "Processing batch 668 (50 docs)...\n",
      "Processing batch 669 (50 docs)...\n",
      "Processing batch 670 (50 docs)...\n",
      "Processing batch 671 (50 docs)...\n",
      "Processing batch 672 (50 docs)...\n",
      "Processing batch 673 (50 docs)...\n",
      "Processing batch 674 (50 docs)...\n",
      "Processing batch 675 (50 docs)...\n",
      "Processing batch 676 (50 docs)...\n",
      "Processing batch 677 (50 docs)...\n",
      "Processing batch 678 (50 docs)...\n",
      "Processing batch 679 (50 docs)...\n",
      "Processing batch 680 (50 docs)...\n",
      "Processing batch 681 (50 docs)...\n",
      "Processing batch 682 (50 docs)...\n",
      "Processing batch 683 (50 docs)...\n",
      "Processing batch 684 (50 docs)...\n",
      "Processing batch 685 (50 docs)...\n",
      "Processing batch 686 (50 docs)...\n",
      "Processing batch 687 (50 docs)...\n",
      "Processing batch 688 (50 docs)...\n",
      "Processing batch 689 (50 docs)...\n",
      "Processing batch 690 (50 docs)...\n",
      "Processing batch 691 (50 docs)...\n",
      "Processing batch 692 (50 docs)...\n",
      "Processing batch 693 (50 docs)...\n",
      "Processing batch 694 (50 docs)...\n",
      "Processing batch 695 (50 docs)...\n",
      "Processing batch 696 (50 docs)...\n",
      "Processing batch 697 (50 docs)...\n",
      "Processing batch 698 (50 docs)...\n",
      "Processing batch 699 (50 docs)...\n",
      "Processing batch 700 (50 docs)...\n",
      "Processing batch 701 (50 docs)...\n",
      "Processing batch 702 (50 docs)...\n",
      "Processing batch 703 (50 docs)...\n",
      "Processing batch 704 (50 docs)...\n",
      "Processing batch 705 (50 docs)...\n",
      "Processing batch 706 (50 docs)...\n",
      "Processing batch 707 (50 docs)...\n",
      "Processing batch 708 (50 docs)...\n",
      "Processing batch 709 (50 docs)...\n",
      "Processing batch 710 (50 docs)...\n",
      "Processing batch 711 (50 docs)...\n",
      "Processing batch 712 (50 docs)...\n",
      "Processing batch 713 (50 docs)...\n",
      "Processing batch 714 (50 docs)...\n",
      "Processing batch 715 (50 docs)...\n",
      "Processing batch 716 (50 docs)...\n",
      "Processing batch 717 (50 docs)...\n",
      "Processing batch 718 (50 docs)...\n",
      "Processing batch 719 (50 docs)...\n",
      "Processing batch 720 (50 docs)...\n",
      "Processing batch 721 (50 docs)...\n",
      "Processing batch 722 (50 docs)...\n",
      "Processing batch 723 (50 docs)...\n",
      "Processing batch 724 (50 docs)...\n",
      "Processing batch 725 (50 docs)...\n",
      "Processing batch 726 (50 docs)...\n",
      "Processing batch 727 (50 docs)...\n",
      "Processing batch 728 (50 docs)...\n",
      "Processing batch 729 (50 docs)...\n",
      "Processing batch 730 (50 docs)...\n",
      "Processing batch 731 (50 docs)...\n",
      "Processing batch 732 (50 docs)...\n",
      "Processing batch 733 (50 docs)...\n",
      "Processing batch 734 (50 docs)...\n",
      "Processing batch 735 (50 docs)...\n",
      "Processing batch 736 (50 docs)...\n",
      "Processing batch 737 (50 docs)...\n",
      "Processing batch 738 (50 docs)...\n",
      "Processing batch 739 (50 docs)...\n",
      "Processing batch 740 (50 docs)...\n",
      "Processing batch 741 (50 docs)...\n",
      "Processing batch 742 (50 docs)...\n",
      "Processing batch 743 (50 docs)...\n",
      "Processing batch 744 (50 docs)...\n",
      "Processing batch 745 (50 docs)...\n",
      "Processing batch 746 (50 docs)...\n",
      "Processing batch 747 (50 docs)...\n",
      "Processing batch 748 (50 docs)...\n",
      "Processing batch 749 (50 docs)...\n",
      "Processing batch 750 (50 docs)...\n",
      "Processing batch 751 (50 docs)...\n",
      "Processing batch 752 (50 docs)...\n",
      "Processing batch 753 (50 docs)...\n",
      "Processing batch 754 (50 docs)...\n",
      "Processing batch 755 (50 docs)...\n",
      "Processing batch 756 (50 docs)...\n",
      "Processing batch 757 (50 docs)...\n",
      "Processing batch 758 (50 docs)...\n",
      "Processing batch 759 (50 docs)...\n",
      "Processing batch 760 (50 docs)...\n",
      "Processing batch 761 (50 docs)...\n",
      "Processing batch 762 (50 docs)...\n",
      "Processing batch 763 (50 docs)...\n",
      "Processing batch 764 (50 docs)...\n",
      "Processing batch 765 (50 docs)...\n",
      "Processing batch 766 (50 docs)...\n",
      "Processing batch 767 (50 docs)...\n",
      "Processing batch 768 (50 docs)...\n",
      "Processing batch 769 (50 docs)...\n",
      "Processing batch 770 (50 docs)...\n",
      "Processing batch 771 (50 docs)...\n",
      "Processing batch 772 (50 docs)...\n",
      "Processing batch 773 (50 docs)...\n",
      "Processing batch 774 (50 docs)...\n",
      "Processing batch 775 (50 docs)...\n",
      "Processing batch 776 (50 docs)...\n",
      "Processing batch 777 (50 docs)...\n",
      "Processing batch 778 (50 docs)...\n",
      "Processing batch 779 (50 docs)...\n",
      "Processing batch 780 (50 docs)...\n",
      "Processing batch 781 (50 docs)...\n",
      "Processing batch 782 (50 docs)...\n",
      "Processing batch 783 (50 docs)...\n",
      "Processing batch 784 (50 docs)...\n",
      "Processing batch 785 (50 docs)...\n",
      "Processing batch 786 (50 docs)...\n",
      "Processing batch 787 (50 docs)...\n",
      "Processing batch 788 (50 docs)...\n",
      "Processing batch 789 (50 docs)...\n",
      "Processing batch 790 (50 docs)...\n",
      "Processing batch 791 (50 docs)...\n",
      "Processing batch 792 (50 docs)...\n",
      "Processing batch 793 (50 docs)...\n",
      "Processing batch 794 (50 docs)...\n",
      "Processing batch 795 (50 docs)...\n",
      "Processing batch 796 (50 docs)...\n",
      "Processing batch 797 (50 docs)...\n",
      "Processing batch 798 (50 docs)...\n",
      "Processing batch 799 (50 docs)...\n",
      "Processing batch 800 (50 docs)...\n",
      "Processing batch 801 (50 docs)...\n",
      "Processing batch 802 (41 docs)...\n",
      "Structured dataset saved to structured_input_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "DATA_DIR = r\"C:\\Users\\islam hitham\\paper_extractor_dataset\\json\"\n",
    "OUTPUT_FILE = \"structured_input_output.jsonl\"\n",
    "BATCH_SIZE = 50  # optional, for processing in batches\n",
    "\n",
    "# ---------------- LOAD & CONVERT ----------------\n",
    "def convert_document(doc):\n",
    "    # Get full text (abstract + body)\n",
    "    full_text = \"\"\n",
    "    # Abstract sentences\n",
    "    abstract_texts = []\n",
    "    for s in doc.get(\"abstract\", []):\n",
    "        if isinstance(s, dict):\n",
    "            abstract_texts.append(s.get(\"text\", \"\"))\n",
    "        else:\n",
    "            abstract_texts.append(str(s))\n",
    "    # Body sentences\n",
    "    body_texts = []\n",
    "    for b in doc.get(\"body_text\", []):\n",
    "        if isinstance(b, dict):\n",
    "            body_texts.append(b.get(\"text\", \"\"))\n",
    "        else:\n",
    "            body_texts.append(str(b))\n",
    "    full_text = \" \".join(abstract_texts + body_texts)\n",
    "\n",
    "    # Everything else is output\n",
    "    output = {k: v for k, v in doc.items() if k != \"abstract\" and k != \"body_text\"}\n",
    "    output[\"abstract\"] = doc.get(\"abstract\", [])\n",
    "    output[\"body_text\"] = doc.get(\"body_text\", [])\n",
    "\n",
    "    return {\"input\": full_text, \"output\": output}\n",
    "\n",
    "def process_all_documents(data_dir, output_file, batch_size=50):\n",
    "    json_files = glob(os.path.join(data_dir, \"**\", \"*.json\"), recursive=True)\n",
    "    print(f\"Found {len(json_files)} JSON files.\")\n",
    "\n",
    "    if len(json_files) == 0:\n",
    "        print(\"No JSON files found. Check the path.\")\n",
    "        return\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for i in range(0, len(json_files), batch_size):\n",
    "            batch_files = json_files[i:i + batch_size]\n",
    "            print(f\"Processing batch {i//batch_size + 1} ({len(batch_files)} docs)...\")\n",
    "            for file_path in batch_files:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    try:\n",
    "                        raw_doc = json.load(f)\n",
    "                        item = convert_document(raw_doc)\n",
    "                        out_f.write(json.dumps(item) + \"\\n\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    print(f\"Structured dataset saved to {output_file}\")\n",
    "\n",
    "# ---------------- RUN ----------------\n",
    "process_all_documents(DATA_DIR, OUTPUT_FILE, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "388d4c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 1 ===\n",
      "\n",
      "--- Input (Full Text) ---\n",
      "O b j e c t i v e   T h e   o v e r a l l   r e s e a r c h   o b j e c t i v e   w a s   t o   t h e o r e t i c a l l y   a n d   e m p i r i c a l l y   d e v e l o p   t h e   i d e a s   a r o u n d   a   s y s t e m   o f   s a f e t y   m a n a g e m e n t   p r a c t i c e s   ( t e n   p r a c t i c e s   w e r e   e l a b o r a t e d ) ,   t o   t e s t   t h e i r   r e l a t i o n s h i p   w i t h   o b j e c t i v e   s a f e t y   s t a t i s t i c s   ( s u c h   a s   a c c i d e n t   r a t e s ) ,   a n d   t o   e x p l o r e   h o w   t h e s e   p r a c t i c e s   w o r k   t o   a c h i e v e   p o s i t i v e   s a f e t y   r e s u l t s   ( a c c i d e n t   p r e v e n t i o n )   t h r o u g h   w o r k e r   e n g a g e m e n t .   M e t h o d   D a t a   w e r e   c o l l e c t e d   u s i n g   s a f e t y   m a n a g e r ,   s u p e r v i s o r   a n d   e m p l o y e e   s u r v e y s   d e s i g n e d   t o   a s s e s s   a n d   l i n k   s a f e t ...\n",
      "\n",
      "--- Output Keys ---\n",
      "['author_highlights', 'bib_entries', 'docId', 'metadata', 'abstract', 'body_text']\n",
      "\n",
      "--- Metadata Sample ---\n",
      "asjc: ['2213', '2739', '3307']\n",
      "authors: [{'email': 'jan.wachter@iup.edu', 'first': 'Jan K.', 'initial': 'J.K.', 'last': 'Wachter'}, {'email': None, 'first': 'Patrick L.', 'initial': 'P.L.', 'last': 'Yorio'}]\n",
      "doi: 10.1016/j.aap.2013.07.029\n",
      "firstpage: 117\n",
      "issn: 00014575\n",
      "keywords: ['Accident prevention', 'Accident rates', 'Human performance', 'Safety management systems', 'Worker engagement']\n",
      "lastpage: 130\n",
      "openaccess: Full\n",
      "pub_year: 2014\n",
      "subjareas: ['ENGI', 'MEDI', 'SOCI']\n",
      "title: A system of safety management practices and worker engagement for reducing and preventing accidents: An empirical and theoretical investigation\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Example 2 ===\n",
      "\n",
      "--- Input (Full Text) ---\n",
      "T h e   o b j e c t i v e   o f   a n   a c c i d e n t - m a p p i n g   a l g o r i t h m   i s   t o   s n a p   t r a f f i c   a c c i d e n t s   o n t o   t h e   c o r r e c t   r o a d   s e g m e n t s .   A s s i g n i n g   a c c i d e n t s   o n t o   t h e   c o r r e c t   s e g m e n t s   f a c i l i t a t e   t o   r o b u s t l y   c a r r y   o u t   s o m e   k e y   a n a l y s e s   i n   a c c i d e n t   r e s e a r c h   i n c l u d i n g   t h e   i d e n t i f i c a t i o n   o f   a c c i d e n t   h o t - s p o t s ,   n e t w o r k - l e v e l   r i s k   m a p p i n g   a n d   s e g m e n t - l e v e l   a c c i d e n t   r i s k   m o d e l l i n g .   E x i s t i n g   r i s k   m a p p i n g   a l g o r i t h m s   h a v e   s o m e   s e v e r e   l i m i t a t i o n s :   ( i )   t h e y   a r e   n o t   e a s i l y   ' t r a n s f e r a b l e '   a s   t h e   a l g o r i t h m s   a r e   s p e c i f i c   t o   g i v e n   a c c i d e n t   d ...\n",
      "\n",
      "--- Output Keys ---\n",
      "['author_highlights', 'bib_entries', 'docId', 'metadata', 'abstract', 'body_text']\n",
      "\n",
      "--- Metadata Sample ---\n",
      "asjc: ['2213', '2739', '3307']\n",
      "authors: [{'email': 'l.deka@lboro.ac.uk', 'first': 'Lipika', 'initial': 'L.', 'last': 'Deka'}, {'email': 'm.a.quddus@lboro.ac.uk', 'first': 'Mohammed', 'initial': 'M.', 'last': 'Quddus'}]\n",
      "doi: 10.1016/j.aap.2013.12.001\n",
      "firstpage: 105\n",
      "issn: 00014575\n",
      "keywords: ['Accident-mapping', 'Artificial neural network', 'Pattern-matching']\n",
      "lastpage: 113\n",
      "openaccess: Full\n",
      "pub_year: 2014\n",
      "subjareas: ['ENGI', 'MEDI', 'SOCI']\n",
      "title: Network-level accident-mapping: Distance based pattern matching using artificial neural network\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "FILE_PATH = \"structured_input_output.jsonl\"\n",
    "\n",
    "# ---------------- SHOW FIRST 2 EXAMPLES ----------------\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(2):\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        doc = json.loads(line)\n",
    "        print(f\"=== Example {i+1} ===\")\n",
    "        print(\"\\n--- Input (Full Text) ---\")\n",
    "        print(doc[\"input\"][:1000] + \"...\" if len(doc[\"input\"]) > 500 else doc[\"input\"])  # first 500 chars\n",
    "        print(\"\\n--- Output Keys ---\")\n",
    "        print(list(doc[\"output\"].keys()))\n",
    "        print(\"\\n--- Metadata Sample ---\")\n",
    "        for k, v in doc[\"output\"].get(\"metadata\", {}).items():\n",
    "            print(f\"{k}: {v}\")\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8577f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed full text for 40091 documents. Saved to structured_input_output_fixed_1.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re \n",
    "\n",
    "INPUT_FILE = \"structured_input_output.jsonl\"\n",
    "OUTPUT_FILE = \"structured_input_output_fixed_1.jsonl\"\n",
    "\n",
    "fixed_count = 0\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as in_f, \\\n",
    "     open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as out_f:\n",
    "\n",
    "    for line in in_f:\n",
    "        try:\n",
    "            doc = json.loads(line)\n",
    "            \n",
    "            # Rebuild full text from abstract + body_text\n",
    "            abstract_texts = []\n",
    "            for s in doc[\"output\"].get(\"abstract\", []):\n",
    "                if isinstance(s, dict):\n",
    "                    abstract_texts.append(s.get(\"text\", \"\"))\n",
    "                else:\n",
    "                    abstract_texts.append(str(s))\n",
    "\n",
    "            body_texts = []\n",
    "            for b in doc[\"output\"].get(\"body_text\", []):\n",
    "                if isinstance(b, dict):\n",
    "                    body_texts.append(b.get(\"text\", \"\"))\n",
    "                else:\n",
    "                    body_texts.append(str(b))\n",
    "\n",
    "            # Join the text fragments; they still contain the bad spacing.\n",
    "            full_text = \" \".join(abstract_texts + body_texts)\n",
    "\n",
    "            # --- START FIXING THE SPACING ---\n",
    "            \n",
    "            # 1. Replace non-breaking spaces ('\\xa0') with standard spaces.\n",
    "            cleaned_text = full_text.replace('\\xa0', ' ')\n",
    "\n",
    "            # 2. Use regex to find sequences of two or more spaces (which separate words) \n",
    "            # and replace them with a unique temporary separator ('|').\n",
    "            # This isolates the words, which still have single internal spaces (e.g., 'O b j e c t i v e').\n",
    "            text_with_separators = re.sub(r'\\s{2,}', '|', cleaned_text).strip()\n",
    "\n",
    "            # 3. Split the text by the separator.\n",
    "            spaced_words = text_with_separators.split('|')\n",
    "            \n",
    "            # 4. For each resulting fragment, remove all remaining internal single spaces (' '), \n",
    "            # effectively collapsing 'O b j e c t i v e' into 'Objective'.\n",
    "            clean_words = [word.replace(' ', '').strip() for word in spaced_words if word.strip()]\n",
    "            \n",
    "            # 5. Join the fully clean words with a single, correct space.\n",
    "            final_full_text = ' '.join(clean_words)\n",
    "            \n",
    "            # --- END FIXING THE SPACING ---\n",
    "            \n",
    "            # Replace input with correctly fixed text\n",
    "            doc[\"input\"] = final_full_text\n",
    "\n",
    "            out_f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "            fixed_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fixing line: {e}\")\n",
    "\n",
    "print(f\"Fixed full text for {fixed_count} documents. Saved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3df61190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 1 ===\n",
      "\n",
      "--- Input (Full Text) ---\n",
      "Objective The overall research objective was to theoretically and empirically develop the ideas around a system of safety management practices (ten practices were elaborated), to test their relationship with objective safety statistics (such as accident rates), and to explore how these practices work to achieve positive safety results (accident prevention) through worker engagement. Method Data were collected using safety manager, supervisor and employee surveys designed to assess and link safety management system practices, employee perceptions resulting from existing practices, and safety performance outcomes. Results Results indicate the following: there is a significant negative relationship between the presence of ten individual safety management practices, as well as the composite of these practices, with accident rates; there is a significant negative relationship between the level of safety-focused worker emotional and cognitive engagement with accident rates; safety management...\n",
      "\n",
      "--- Output Keys ---\n",
      "['author_highlights', 'bib_entries', 'docId', 'metadata', 'abstract', 'body_text']\n",
      "\n",
      "--- Metadata Sample ---\n",
      "asjc: ['2213', '2739', '3307']\n",
      "authors: [{'email': 'jan.wachter@iup.edu', 'first': 'Jan K.', 'initial': 'J.K.', 'last': 'Wachter'}, {'email': None, 'first': 'Patrick L.', 'initial': 'P.L.', 'last': 'Yorio'}]\n",
      "doi: 10.1016/j.aap.2013.07.029\n",
      "firstpage: 117\n",
      "issn: 00014575\n",
      "keywords: ['Accident prevention', 'Accident rates', 'Human performance', 'Safety management systems', 'Worker engagement']\n",
      "lastpage: 130\n",
      "openaccess: Full\n",
      "pub_year: 2014\n",
      "subjareas: ['ENGI', 'MEDI', 'SOCI']\n",
      "title: A system of safety management practices and worker engagement for reducing and preventing accidents: An empirical and theoretical investigation\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Example 2 ===\n",
      "\n",
      "--- Input (Full Text) ---\n",
      "The objective of an accident-mapping algorithm is to snap traffic accidents onto the correct road segments. Assigning accidents onto the correct segments facilitate to robustly carry out some key analyses in accident research including the identification of accident hot-spots, network-level risk mapping and segment-level accident risk modelling. Existing risk mapping algorithms have some severe limitations: (i) they are not easily 'transferable' as the algorithms are specific to given accident datasets; (ii) they do not perform well in all road-network environments such as in areas of dense road network; and (iii) the methods used do not perform well in addressing inaccuracies inherent in and type of road environment. The purpose of this paper is to develop a new accident mapping algorithm based on the common variables observed in most accident databases (e.g. road name and type, direction of vehicle movement before the accident and recorded accident location). The challenges here are ...\n",
      "\n",
      "--- Output Keys ---\n",
      "['author_highlights', 'bib_entries', 'docId', 'metadata', 'abstract', 'body_text']\n",
      "\n",
      "--- Metadata Sample ---\n",
      "asjc: ['2213', '2739', '3307']\n",
      "authors: [{'email': 'l.deka@lboro.ac.uk', 'first': 'Lipika', 'initial': 'L.', 'last': 'Deka'}, {'email': 'm.a.quddus@lboro.ac.uk', 'first': 'Mohammed', 'initial': 'M.', 'last': 'Quddus'}]\n",
      "doi: 10.1016/j.aap.2013.12.001\n",
      "firstpage: 105\n",
      "issn: 00014575\n",
      "keywords: ['Accident-mapping', 'Artificial neural network', 'Pattern-matching']\n",
      "lastpage: 113\n",
      "openaccess: Full\n",
      "pub_year: 2014\n",
      "subjareas: ['ENGI', 'MEDI', 'SOCI']\n",
      "title: Network-level accident-mapping: Distance based pattern matching using artificial neural network\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "FILE_PATH = \"structured_input_output_fixed_1.jsonl\"\n",
    "\n",
    "# ---------------- SHOW FIRST 2 EXAMPLES ----------------\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(2):\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        doc = json.loads(line)\n",
    "        print(f\"=== Example {i+1} ===\")\n",
    "        print(\"\\n--- Input (Full Text) ---\")\n",
    "        print(doc[\"input\"][:1000] + \"...\" if len(doc[\"input\"]) > 500 else doc[\"input\"])  # first 500 chars\n",
    "        print(\"\\n--- Output Keys ---\")\n",
    "        print(list(doc[\"output\"].keys()))\n",
    "        print(\"\\n--- Metadata Sample ---\")\n",
    "        for k, v in doc[\"output\"].get(\"metadata\", {}).items():\n",
    "            print(f\"{k}: {v}\")\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348ea5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd202836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c04f162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "import evaluate\n",
    "import json\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcfc3e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Example 1 \n",
      "Input (first 300 chars): Objective The overall research objective was to theoretically and empirically develop the ideas around a system of safety management practices (ten practices were elaborated), to test their relationship with objective safety statistics (such as accident rates), and to explore how these practices wor\n",
      "Output keys: ['author_highlights', 'bib_entries', 'docId', 'metadata', 'abstract', 'body_text']\n",
      "\n",
      " Example 2 \n",
      "Input (first 300 chars): The objective of an accident-mapping algorithm is to snap traffic accidents onto the correct road segments. Assigning accidents onto the correct segments facilitate to robustly carry out some key analyses in accident research including the identification of accident hot-spots, network-level risk map\n",
      "Output keys: ['author_highlights', 'bib_entries', 'docId', 'metadata', 'abstract', 'body_text']\n",
      "\n",
      " Example 3 \n",
      "Input (first 300 chars): The Driver Behavior Questionnaire (DBQ) is a self-report measure of driving behavior that has been widely used over more than 20 years. Despite this wealth of evidence a number of questions remain, including understanding the correlation between its violations and errors sub-components, identifying \n",
      "Output keys: ['author_highlights', 'bib_entries', 'docId', 'metadata', 'abstract', 'body_text']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "import json\n",
    "\n",
    "file_path = r\"C:\\Users\\islam hitham\\Paper-LLM-Extractor\\notebooks\\structured_input_output_fixed_1.jsonl\"\n",
    "\n",
    "# Streaming load\n",
    "dataset = load_dataset(\"json\", data_files=file_path, split=\"train\", streaming=True)\n",
    "\n",
    "# Iterate safely, skip empty/malformed lines\n",
    "examples = []\n",
    "for i, item in enumerate(islice(dataset, 3)):\n",
    "    # Sometimes streaming returns string or dict\n",
    "    if isinstance(item, str):\n",
    "        item = item.strip()\n",
    "        if not item:  # skip blank lines\n",
    "            continue\n",
    "        try:\n",
    "            item = json.loads(item)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Skipping malformed line {i+1}\")\n",
    "            continue\n",
    "    elif not item:\n",
    "        continue\n",
    "\n",
    "    examples.append(item)\n",
    "    print(f\" Example {len(examples)} \")\n",
    "    print(\"Input (first 300 chars):\", item[\"input\"][:300])\n",
    "    print(\"Output keys:\", list(item[\"output\"].keys()))\n",
    "    print()\n",
    "\n",
    "if not examples:\n",
    "    print(\"No valid examples found. Check your JSONL file for empty or malformed lines.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c82dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"models/llama-3.2-1b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def format_text(example):\n",
    "    return {\"text\": example[\"text\"]}\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    texts = [t for t in examples[\"text\"]]\n",
    "    return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=1024)\n",
    "\n",
    "dataset = dataset.map(format_text)\n",
    "tokenized_dataset = dataset.map(tokenize_fn, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "print(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de750a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c163ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    results = rouge.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    # Basic JSON structure accuracy\n",
    "    correct_json = 0\n",
    "    for p in pred_str:\n",
    "        try:\n",
    "            json.loads(p)\n",
    "            correct_json += 1\n",
    "        except:\n",
    "            pass\n",
    "    json_acc = correct_json / len(pred_str)\n",
    "    \n",
    "    return {\"rougeL\": results[\"rougeL\"], \"json_accuracy\": json_acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f3c642",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_llama3b_output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    warmup_steps=20,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./qlora_llama3b_adapter\")\n",
    "tokenizer.save_pretrained(\"./qlora_llama3b_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e66c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py330",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
