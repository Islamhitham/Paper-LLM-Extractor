{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c54f16",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50d39a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40091 JSON files.\n",
      "Processing batch 1 (50 docs)...\n",
      "Processing batch 2 (50 docs)...\n",
      "Processing batch 3 (50 docs)...\n",
      "Processing batch 4 (50 docs)...\n",
      "Processing batch 5 (50 docs)...\n",
      "Processing batch 6 (50 docs)...\n",
      "Processing batch 7 (50 docs)...\n",
      "Processing batch 8 (50 docs)...\n",
      "Processing batch 9 (50 docs)...\n",
      "Processing batch 10 (50 docs)...\n",
      "Processing batch 11 (50 docs)...\n",
      "Processing batch 12 (50 docs)...\n",
      "Processing batch 13 (50 docs)...\n",
      "Processing batch 14 (50 docs)...\n",
      "Processing batch 15 (50 docs)...\n",
      "Processing batch 16 (50 docs)...\n",
      "Processing batch 17 (50 docs)...\n",
      "Processing batch 18 (50 docs)...\n",
      "Processing batch 19 (50 docs)...\n",
      "Processing batch 20 (50 docs)...\n",
      "Processing batch 21 (50 docs)...\n",
      "Processing batch 22 (50 docs)...\n",
      "Processing batch 23 (50 docs)...\n",
      "Processing batch 24 (50 docs)...\n",
      "Processing batch 25 (50 docs)...\n",
      "Processing batch 26 (50 docs)...\n",
      "Processing batch 27 (50 docs)...\n",
      "Processing batch 28 (50 docs)...\n",
      "Processing batch 29 (50 docs)...\n",
      "Processing batch 30 (50 docs)...\n",
      "Processing batch 31 (50 docs)...\n",
      "Processing batch 32 (50 docs)...\n",
      "Processing batch 33 (50 docs)...\n",
      "Processing batch 34 (50 docs)...\n",
      "Processing batch 35 (50 docs)...\n",
      "Processing batch 36 (50 docs)...\n",
      "Processing batch 37 (50 docs)...\n",
      "Processing batch 38 (50 docs)...\n",
      "Processing batch 39 (50 docs)...\n",
      "Processing batch 40 (50 docs)...\n",
      "Processing batch 41 (50 docs)...\n",
      "Processing batch 42 (50 docs)...\n",
      "Processing batch 43 (50 docs)...\n",
      "Processing batch 44 (50 docs)...\n",
      "Processing batch 45 (50 docs)...\n",
      "Processing batch 46 (50 docs)...\n",
      "Processing batch 47 (50 docs)...\n",
      "Processing batch 48 (50 docs)...\n",
      "Processing batch 49 (50 docs)...\n",
      "Processing batch 50 (50 docs)...\n",
      "Processing batch 51 (50 docs)...\n",
      "Processing batch 52 (50 docs)...\n",
      "Processing batch 53 (50 docs)...\n",
      "Processing batch 54 (50 docs)...\n",
      "Processing batch 55 (50 docs)...\n",
      "Processing batch 56 (50 docs)...\n",
      "Processing batch 57 (50 docs)...\n",
      "Processing batch 58 (50 docs)...\n",
      "Processing batch 59 (50 docs)...\n",
      "Processing batch 60 (50 docs)...\n",
      "Processing batch 61 (50 docs)...\n",
      "Processing batch 62 (50 docs)...\n",
      "Processing batch 63 (50 docs)...\n",
      "Processing batch 64 (50 docs)...\n",
      "Processing batch 65 (50 docs)...\n",
      "Processing batch 66 (50 docs)...\n",
      "Processing batch 67 (50 docs)...\n",
      "Processing batch 68 (50 docs)...\n",
      "Processing batch 69 (50 docs)...\n",
      "Processing batch 70 (50 docs)...\n",
      "Processing batch 71 (50 docs)...\n",
      "Processing batch 72 (50 docs)...\n",
      "Processing batch 73 (50 docs)...\n",
      "Processing batch 74 (50 docs)...\n",
      "Processing batch 75 (50 docs)...\n",
      "Processing batch 76 (50 docs)...\n",
      "Processing batch 77 (50 docs)...\n",
      "Processing batch 78 (50 docs)...\n",
      "Processing batch 79 (50 docs)...\n",
      "Processing batch 80 (50 docs)...\n",
      "Processing batch 81 (50 docs)...\n",
      "Processing batch 82 (50 docs)...\n",
      "Processing batch 83 (50 docs)...\n",
      "Processing batch 84 (50 docs)...\n",
      "Processing batch 85 (50 docs)...\n",
      "Processing batch 86 (50 docs)...\n",
      "Processing batch 87 (50 docs)...\n",
      "Processing batch 88 (50 docs)...\n",
      "Processing batch 89 (50 docs)...\n",
      "Processing batch 90 (50 docs)...\n",
      "Processing batch 91 (50 docs)...\n",
      "Processing batch 92 (50 docs)...\n",
      "Processing batch 93 (50 docs)...\n",
      "Processing batch 94 (50 docs)...\n",
      "Processing batch 95 (50 docs)...\n",
      "Processing batch 96 (50 docs)...\n",
      "Processing batch 97 (50 docs)...\n",
      "Processing batch 98 (50 docs)...\n",
      "Processing batch 99 (50 docs)...\n",
      "Processing batch 100 (50 docs)...\n",
      "Processing batch 101 (50 docs)...\n",
      "Processing batch 102 (50 docs)...\n",
      "Processing batch 103 (50 docs)...\n",
      "Processing batch 104 (50 docs)...\n",
      "Processing batch 105 (50 docs)...\n",
      "Processing batch 106 (50 docs)...\n",
      "Processing batch 107 (50 docs)...\n",
      "Processing batch 108 (50 docs)...\n",
      "Processing batch 109 (50 docs)...\n",
      "Processing batch 110 (50 docs)...\n",
      "Processing batch 111 (50 docs)...\n",
      "Processing batch 112 (50 docs)...\n",
      "Processing batch 113 (50 docs)...\n",
      "Processing batch 114 (50 docs)...\n",
      "Processing batch 115 (50 docs)...\n",
      "Processing batch 116 (50 docs)...\n",
      "Processing batch 117 (50 docs)...\n",
      "Processing batch 118 (50 docs)...\n",
      "Processing batch 119 (50 docs)...\n",
      "Processing batch 120 (50 docs)...\n",
      "Processing batch 121 (50 docs)...\n",
      "Processing batch 122 (50 docs)...\n",
      "Processing batch 123 (50 docs)...\n",
      "Processing batch 124 (50 docs)...\n",
      "Processing batch 125 (50 docs)...\n",
      "Processing batch 126 (50 docs)...\n",
      "Processing batch 127 (50 docs)...\n",
      "Processing batch 128 (50 docs)...\n",
      "Processing batch 129 (50 docs)...\n",
      "Processing batch 130 (50 docs)...\n",
      "Processing batch 131 (50 docs)...\n",
      "Processing batch 132 (50 docs)...\n",
      "Processing batch 133 (50 docs)...\n",
      "Processing batch 134 (50 docs)...\n",
      "Processing batch 135 (50 docs)...\n",
      "Processing batch 136 (50 docs)...\n",
      "Processing batch 137 (50 docs)...\n",
      "Processing batch 138 (50 docs)...\n",
      "Processing batch 139 (50 docs)...\n",
      "Processing batch 140 (50 docs)...\n",
      "Processing batch 141 (50 docs)...\n",
      "Processing batch 142 (50 docs)...\n",
      "Processing batch 143 (50 docs)...\n",
      "Processing batch 144 (50 docs)...\n",
      "Processing batch 145 (50 docs)...\n",
      "Processing batch 146 (50 docs)...\n",
      "Processing batch 147 (50 docs)...\n",
      "Processing batch 148 (50 docs)...\n",
      "Processing batch 149 (50 docs)...\n",
      "Processing batch 150 (50 docs)...\n",
      "Processing batch 151 (50 docs)...\n",
      "Processing batch 152 (50 docs)...\n",
      "Processing batch 153 (50 docs)...\n",
      "Processing batch 154 (50 docs)...\n",
      "Processing batch 155 (50 docs)...\n",
      "Processing batch 156 (50 docs)...\n",
      "Processing batch 157 (50 docs)...\n",
      "Processing batch 158 (50 docs)...\n",
      "Processing batch 159 (50 docs)...\n",
      "Processing batch 160 (50 docs)...\n",
      "Processing batch 161 (50 docs)...\n",
      "Processing batch 162 (50 docs)...\n",
      "Processing batch 163 (50 docs)...\n",
      "Processing batch 164 (50 docs)...\n",
      "Processing batch 165 (50 docs)...\n",
      "Processing batch 166 (50 docs)...\n",
      "Processing batch 167 (50 docs)...\n",
      "Processing batch 168 (50 docs)...\n",
      "Processing batch 169 (50 docs)...\n",
      "Processing batch 170 (50 docs)...\n",
      "Processing batch 171 (50 docs)...\n",
      "Processing batch 172 (50 docs)...\n",
      "Processing batch 173 (50 docs)...\n",
      "Processing batch 174 (50 docs)...\n",
      "Processing batch 175 (50 docs)...\n",
      "Processing batch 176 (50 docs)...\n",
      "Processing batch 177 (50 docs)...\n",
      "Processing batch 178 (50 docs)...\n",
      "Processing batch 179 (50 docs)...\n",
      "Processing batch 180 (50 docs)...\n",
      "Processing batch 181 (50 docs)...\n",
      "Processing batch 182 (50 docs)...\n",
      "Processing batch 183 (50 docs)...\n",
      "Processing batch 184 (50 docs)...\n",
      "Processing batch 185 (50 docs)...\n",
      "Processing batch 186 (50 docs)...\n",
      "Processing batch 187 (50 docs)...\n",
      "Processing batch 188 (50 docs)...\n",
      "Processing batch 189 (50 docs)...\n",
      "Processing batch 190 (50 docs)...\n",
      "Processing batch 191 (50 docs)...\n",
      "Processing batch 192 (50 docs)...\n",
      "Processing batch 193 (50 docs)...\n",
      "Processing batch 194 (50 docs)...\n",
      "Processing batch 195 (50 docs)...\n",
      "Processing batch 196 (50 docs)...\n",
      "Processing batch 197 (50 docs)...\n",
      "Processing batch 198 (50 docs)...\n",
      "Processing batch 199 (50 docs)...\n",
      "Processing batch 200 (50 docs)...\n",
      "Processing batch 201 (50 docs)...\n",
      "Processing batch 202 (50 docs)...\n",
      "Processing batch 203 (50 docs)...\n",
      "Processing batch 204 (50 docs)...\n",
      "Processing batch 205 (50 docs)...\n",
      "Processing batch 206 (50 docs)...\n",
      "Processing batch 207 (50 docs)...\n",
      "Processing batch 208 (50 docs)...\n",
      "Processing batch 209 (50 docs)...\n",
      "Processing batch 210 (50 docs)...\n",
      "Processing batch 211 (50 docs)...\n",
      "Processing batch 212 (50 docs)...\n",
      "Processing batch 213 (50 docs)...\n",
      "Processing batch 214 (50 docs)...\n",
      "Processing batch 215 (50 docs)...\n",
      "Processing batch 216 (50 docs)...\n",
      "Processing batch 217 (50 docs)...\n",
      "Processing batch 218 (50 docs)...\n",
      "Processing batch 219 (50 docs)...\n",
      "Processing batch 220 (50 docs)...\n",
      "Processing batch 221 (50 docs)...\n",
      "Processing batch 222 (50 docs)...\n",
      "Processing batch 223 (50 docs)...\n",
      "Processing batch 224 (50 docs)...\n",
      "Processing batch 225 (50 docs)...\n",
      "Processing batch 226 (50 docs)...\n",
      "Processing batch 227 (50 docs)...\n",
      "Processing batch 228 (50 docs)...\n",
      "Processing batch 229 (50 docs)...\n",
      "Processing batch 230 (50 docs)...\n",
      "Processing batch 231 (50 docs)...\n",
      "Processing batch 232 (50 docs)...\n",
      "Processing batch 233 (50 docs)...\n",
      "Processing batch 234 (50 docs)...\n",
      "Processing batch 235 (50 docs)...\n",
      "Processing batch 236 (50 docs)...\n",
      "Processing batch 237 (50 docs)...\n",
      "Processing batch 238 (50 docs)...\n",
      "Processing batch 239 (50 docs)...\n",
      "Processing batch 240 (50 docs)...\n",
      "Processing batch 241 (50 docs)...\n",
      "Processing batch 242 (50 docs)...\n",
      "Processing batch 243 (50 docs)...\n",
      "Processing batch 244 (50 docs)...\n",
      "Processing batch 245 (50 docs)...\n",
      "Processing batch 246 (50 docs)...\n",
      "Processing batch 247 (50 docs)...\n",
      "Processing batch 248 (50 docs)...\n",
      "Processing batch 249 (50 docs)...\n",
      "Processing batch 250 (50 docs)...\n",
      "Processing batch 251 (50 docs)...\n",
      "Processing batch 252 (50 docs)...\n",
      "Processing batch 253 (50 docs)...\n",
      "Processing batch 254 (50 docs)...\n",
      "Processing batch 255 (50 docs)...\n",
      "Processing batch 256 (50 docs)...\n",
      "Processing batch 257 (50 docs)...\n",
      "Processing batch 258 (50 docs)...\n",
      "Processing batch 259 (50 docs)...\n",
      "Processing batch 260 (50 docs)...\n",
      "Processing batch 261 (50 docs)...\n",
      "Processing batch 262 (50 docs)...\n",
      "Processing batch 263 (50 docs)...\n",
      "Processing batch 264 (50 docs)...\n",
      "Processing batch 265 (50 docs)...\n",
      "Processing batch 266 (50 docs)...\n",
      "Processing batch 267 (50 docs)...\n",
      "Processing batch 268 (50 docs)...\n",
      "Processing batch 269 (50 docs)...\n",
      "Processing batch 270 (50 docs)...\n",
      "Processing batch 271 (50 docs)...\n",
      "Processing batch 272 (50 docs)...\n",
      "Processing batch 273 (50 docs)...\n",
      "Processing batch 274 (50 docs)...\n",
      "Processing batch 275 (50 docs)...\n",
      "Processing batch 276 (50 docs)...\n",
      "Processing batch 277 (50 docs)...\n",
      "Processing batch 278 (50 docs)...\n",
      "Processing batch 279 (50 docs)...\n",
      "Processing batch 280 (50 docs)...\n",
      "Processing batch 281 (50 docs)...\n",
      "Processing batch 282 (50 docs)...\n",
      "Processing batch 283 (50 docs)...\n",
      "Processing batch 284 (50 docs)...\n",
      "Processing batch 285 (50 docs)...\n",
      "Processing batch 286 (50 docs)...\n",
      "Processing batch 287 (50 docs)...\n",
      "Processing batch 288 (50 docs)...\n",
      "Processing batch 289 (50 docs)...\n",
      "Processing batch 290 (50 docs)...\n",
      "Processing batch 291 (50 docs)...\n",
      "Processing batch 292 (50 docs)...\n",
      "Processing batch 293 (50 docs)...\n",
      "Processing batch 294 (50 docs)...\n",
      "Processing batch 295 (50 docs)...\n",
      "Processing batch 296 (50 docs)...\n",
      "Processing batch 297 (50 docs)...\n",
      "Processing batch 298 (50 docs)...\n",
      "Processing batch 299 (50 docs)...\n",
      "Processing batch 300 (50 docs)...\n",
      "Processing batch 301 (50 docs)...\n",
      "Processing batch 302 (50 docs)...\n",
      "Processing batch 303 (50 docs)...\n",
      "Processing batch 304 (50 docs)...\n",
      "Processing batch 305 (50 docs)...\n",
      "Processing batch 306 (50 docs)...\n",
      "Processing batch 307 (50 docs)...\n",
      "Processing batch 308 (50 docs)...\n",
      "Processing batch 309 (50 docs)...\n",
      "Processing batch 310 (50 docs)...\n",
      "Processing batch 311 (50 docs)...\n",
      "Processing batch 312 (50 docs)...\n",
      "Processing batch 313 (50 docs)...\n",
      "Processing batch 314 (50 docs)...\n",
      "Processing batch 315 (50 docs)...\n",
      "Processing batch 316 (50 docs)...\n",
      "Processing batch 317 (50 docs)...\n",
      "Processing batch 318 (50 docs)...\n",
      "Processing batch 319 (50 docs)...\n",
      "Processing batch 320 (50 docs)...\n",
      "Processing batch 321 (50 docs)...\n",
      "Processing batch 322 (50 docs)...\n",
      "Processing batch 323 (50 docs)...\n",
      "Processing batch 324 (50 docs)...\n",
      "Processing batch 325 (50 docs)...\n",
      "Processing batch 326 (50 docs)...\n",
      "Processing batch 327 (50 docs)...\n",
      "Processing batch 328 (50 docs)...\n",
      "Processing batch 329 (50 docs)...\n",
      "Processing batch 330 (50 docs)...\n",
      "Processing batch 331 (50 docs)...\n",
      "Processing batch 332 (50 docs)...\n",
      "Processing batch 333 (50 docs)...\n",
      "Processing batch 334 (50 docs)...\n",
      "Processing batch 335 (50 docs)...\n",
      "Processing batch 336 (50 docs)...\n",
      "Processing batch 337 (50 docs)...\n",
      "Processing batch 338 (50 docs)...\n",
      "Processing batch 339 (50 docs)...\n",
      "Processing batch 340 (50 docs)...\n",
      "Processing batch 341 (50 docs)...\n",
      "Processing batch 342 (50 docs)...\n",
      "Processing batch 343 (50 docs)...\n",
      "Processing batch 344 (50 docs)...\n",
      "Processing batch 345 (50 docs)...\n",
      "Processing batch 346 (50 docs)...\n",
      "Processing batch 347 (50 docs)...\n",
      "Processing batch 348 (50 docs)...\n",
      "Processing batch 349 (50 docs)...\n",
      "Processing batch 350 (50 docs)...\n",
      "Processing batch 351 (50 docs)...\n",
      "Processing batch 352 (50 docs)...\n",
      "Processing batch 353 (50 docs)...\n",
      "Processing batch 354 (50 docs)...\n",
      "Processing batch 355 (50 docs)...\n",
      "Processing batch 356 (50 docs)...\n",
      "Processing batch 357 (50 docs)...\n",
      "Processing batch 358 (50 docs)...\n",
      "Processing batch 359 (50 docs)...\n",
      "Processing batch 360 (50 docs)...\n",
      "Processing batch 361 (50 docs)...\n",
      "Processing batch 362 (50 docs)...\n",
      "Processing batch 363 (50 docs)...\n",
      "Processing batch 364 (50 docs)...\n",
      "Processing batch 365 (50 docs)...\n",
      "Processing batch 366 (50 docs)...\n",
      "Processing batch 367 (50 docs)...\n",
      "Processing batch 368 (50 docs)...\n",
      "Processing batch 369 (50 docs)...\n",
      "Processing batch 370 (50 docs)...\n",
      "Processing batch 371 (50 docs)...\n",
      "Processing batch 372 (50 docs)...\n",
      "Processing batch 373 (50 docs)...\n",
      "Processing batch 374 (50 docs)...\n",
      "Processing batch 375 (50 docs)...\n",
      "Processing batch 376 (50 docs)...\n",
      "Processing batch 377 (50 docs)...\n",
      "Processing batch 378 (50 docs)...\n",
      "Processing batch 379 (50 docs)...\n",
      "Processing batch 380 (50 docs)...\n",
      "Processing batch 381 (50 docs)...\n",
      "Processing batch 382 (50 docs)...\n",
      "Processing batch 383 (50 docs)...\n",
      "Processing batch 384 (50 docs)...\n",
      "Processing batch 385 (50 docs)...\n",
      "Processing batch 386 (50 docs)...\n",
      "Processing batch 387 (50 docs)...\n",
      "Processing batch 388 (50 docs)...\n",
      "Processing batch 389 (50 docs)...\n",
      "Processing batch 390 (50 docs)...\n",
      "Processing batch 391 (50 docs)...\n",
      "Processing batch 392 (50 docs)...\n",
      "Processing batch 393 (50 docs)...\n",
      "Processing batch 394 (50 docs)...\n",
      "Processing batch 395 (50 docs)...\n",
      "Processing batch 396 (50 docs)...\n",
      "Processing batch 397 (50 docs)...\n",
      "Processing batch 398 (50 docs)...\n",
      "Processing batch 399 (50 docs)...\n",
      "Processing batch 400 (50 docs)...\n",
      "Processing batch 401 (50 docs)...\n",
      "Processing batch 402 (50 docs)...\n",
      "Processing batch 403 (50 docs)...\n",
      "Processing batch 404 (50 docs)...\n",
      "Processing batch 405 (50 docs)...\n",
      "Processing batch 406 (50 docs)...\n",
      "Processing batch 407 (50 docs)...\n",
      "Processing batch 408 (50 docs)...\n",
      "Processing batch 409 (50 docs)...\n",
      "Processing batch 410 (50 docs)...\n",
      "Processing batch 411 (50 docs)...\n",
      "Processing batch 412 (50 docs)...\n",
      "Processing batch 413 (50 docs)...\n",
      "Processing batch 414 (50 docs)...\n",
      "Processing batch 415 (50 docs)...\n",
      "Processing batch 416 (50 docs)...\n",
      "Processing batch 417 (50 docs)...\n",
      "Processing batch 418 (50 docs)...\n",
      "Processing batch 419 (50 docs)...\n",
      "Processing batch 420 (50 docs)...\n",
      "Processing batch 421 (50 docs)...\n",
      "Processing batch 422 (50 docs)...\n",
      "Processing batch 423 (50 docs)...\n",
      "Processing batch 424 (50 docs)...\n",
      "Processing batch 425 (50 docs)...\n",
      "Processing batch 426 (50 docs)...\n",
      "Processing batch 427 (50 docs)...\n",
      "Processing batch 428 (50 docs)...\n",
      "Processing batch 429 (50 docs)...\n",
      "Processing batch 430 (50 docs)...\n",
      "Processing batch 431 (50 docs)...\n",
      "Processing batch 432 (50 docs)...\n",
      "Processing batch 433 (50 docs)...\n",
      "Processing batch 434 (50 docs)...\n",
      "Processing batch 435 (50 docs)...\n",
      "Processing batch 436 (50 docs)...\n",
      "Processing batch 437 (50 docs)...\n",
      "Processing batch 438 (50 docs)...\n",
      "Processing batch 439 (50 docs)...\n",
      "Processing batch 440 (50 docs)...\n",
      "Processing batch 441 (50 docs)...\n",
      "Processing batch 442 (50 docs)...\n",
      "Processing batch 443 (50 docs)...\n",
      "Processing batch 444 (50 docs)...\n",
      "Processing batch 445 (50 docs)...\n",
      "Processing batch 446 (50 docs)...\n",
      "Processing batch 447 (50 docs)...\n",
      "Processing batch 448 (50 docs)...\n",
      "Processing batch 449 (50 docs)...\n",
      "Processing batch 450 (50 docs)...\n",
      "Processing batch 451 (50 docs)...\n",
      "Processing batch 452 (50 docs)...\n",
      "Processing batch 453 (50 docs)...\n",
      "Processing batch 454 (50 docs)...\n",
      "Processing batch 455 (50 docs)...\n",
      "Processing batch 456 (50 docs)...\n",
      "Processing batch 457 (50 docs)...\n",
      "Processing batch 458 (50 docs)...\n",
      "Processing batch 459 (50 docs)...\n",
      "Processing batch 460 (50 docs)...\n",
      "Processing batch 461 (50 docs)...\n",
      "Processing batch 462 (50 docs)...\n",
      "Processing batch 463 (50 docs)...\n",
      "Processing batch 464 (50 docs)...\n",
      "Processing batch 465 (50 docs)...\n",
      "Processing batch 466 (50 docs)...\n",
      "Processing batch 467 (50 docs)...\n",
      "Processing batch 468 (50 docs)...\n",
      "Processing batch 469 (50 docs)...\n",
      "Processing batch 470 (50 docs)...\n",
      "Processing batch 471 (50 docs)...\n",
      "Processing batch 472 (50 docs)...\n",
      "Processing batch 473 (50 docs)...\n",
      "Processing batch 474 (50 docs)...\n",
      "Processing batch 475 (50 docs)...\n",
      "Processing batch 476 (50 docs)...\n",
      "Processing batch 477 (50 docs)...\n",
      "Processing batch 478 (50 docs)...\n",
      "Processing batch 479 (50 docs)...\n",
      "Processing batch 480 (50 docs)...\n",
      "Processing batch 481 (50 docs)...\n",
      "Processing batch 482 (50 docs)...\n",
      "Processing batch 483 (50 docs)...\n",
      "Processing batch 484 (50 docs)...\n",
      "Processing batch 485 (50 docs)...\n",
      "Processing batch 486 (50 docs)...\n",
      "Processing batch 487 (50 docs)...\n",
      "Processing batch 488 (50 docs)...\n",
      "Processing batch 489 (50 docs)...\n",
      "Processing batch 490 (50 docs)...\n",
      "Processing batch 491 (50 docs)...\n",
      "Processing batch 492 (50 docs)...\n",
      "Processing batch 493 (50 docs)...\n",
      "Processing batch 494 (50 docs)...\n",
      "Processing batch 495 (50 docs)...\n",
      "Processing batch 496 (50 docs)...\n",
      "Processing batch 497 (50 docs)...\n",
      "Processing batch 498 (50 docs)...\n",
      "Processing batch 499 (50 docs)...\n",
      "Processing batch 500 (50 docs)...\n",
      "Processing batch 501 (50 docs)...\n",
      "Processing batch 502 (50 docs)...\n",
      "Processing batch 503 (50 docs)...\n",
      "Processing batch 504 (50 docs)...\n",
      "Processing batch 505 (50 docs)...\n",
      "Processing batch 506 (50 docs)...\n",
      "Processing batch 507 (50 docs)...\n",
      "Processing batch 508 (50 docs)...\n",
      "Processing batch 509 (50 docs)...\n",
      "Processing batch 510 (50 docs)...\n",
      "Processing batch 511 (50 docs)...\n",
      "Processing batch 512 (50 docs)...\n",
      "Processing batch 513 (50 docs)...\n",
      "Processing batch 514 (50 docs)...\n",
      "Processing batch 515 (50 docs)...\n",
      "Processing batch 516 (50 docs)...\n",
      "Processing batch 517 (50 docs)...\n",
      "Processing batch 518 (50 docs)...\n",
      "Processing batch 519 (50 docs)...\n",
      "Processing batch 520 (50 docs)...\n",
      "Processing batch 521 (50 docs)...\n",
      "Processing batch 522 (50 docs)...\n",
      "Processing batch 523 (50 docs)...\n",
      "Processing batch 524 (50 docs)...\n",
      "Processing batch 525 (50 docs)...\n",
      "Processing batch 526 (50 docs)...\n",
      "Processing batch 527 (50 docs)...\n",
      "Processing batch 528 (50 docs)...\n",
      "Processing batch 529 (50 docs)...\n",
      "Processing batch 530 (50 docs)...\n",
      "Processing batch 531 (50 docs)...\n",
      "Processing batch 532 (50 docs)...\n",
      "Processing batch 533 (50 docs)...\n",
      "Processing batch 534 (50 docs)...\n",
      "Processing batch 535 (50 docs)...\n",
      "Processing batch 536 (50 docs)...\n",
      "Processing batch 537 (50 docs)...\n",
      "Processing batch 538 (50 docs)...\n",
      "Processing batch 539 (50 docs)...\n",
      "Processing batch 540 (50 docs)...\n",
      "Processing batch 541 (50 docs)...\n",
      "Processing batch 542 (50 docs)...\n",
      "Processing batch 543 (50 docs)...\n",
      "Processing batch 544 (50 docs)...\n",
      "Processing batch 545 (50 docs)...\n",
      "Processing batch 546 (50 docs)...\n",
      "Processing batch 547 (50 docs)...\n",
      "Processing batch 548 (50 docs)...\n",
      "Processing batch 549 (50 docs)...\n",
      "Processing batch 550 (50 docs)...\n",
      "Processing batch 551 (50 docs)...\n",
      "Processing batch 552 (50 docs)...\n",
      "Processing batch 553 (50 docs)...\n",
      "Processing batch 554 (50 docs)...\n",
      "Processing batch 555 (50 docs)...\n",
      "Processing batch 556 (50 docs)...\n",
      "Processing batch 557 (50 docs)...\n",
      "Processing batch 558 (50 docs)...\n",
      "Processing batch 559 (50 docs)...\n",
      "Processing batch 560 (50 docs)...\n",
      "Processing batch 561 (50 docs)...\n",
      "Processing batch 562 (50 docs)...\n",
      "Processing batch 563 (50 docs)...\n",
      "Processing batch 564 (50 docs)...\n",
      "Processing batch 565 (50 docs)...\n",
      "Processing batch 566 (50 docs)...\n",
      "Processing batch 567 (50 docs)...\n",
      "Processing batch 568 (50 docs)...\n",
      "Processing batch 569 (50 docs)...\n",
      "Processing batch 570 (50 docs)...\n",
      "Processing batch 571 (50 docs)...\n",
      "Processing batch 572 (50 docs)...\n",
      "Processing batch 573 (50 docs)...\n",
      "Processing batch 574 (50 docs)...\n",
      "Processing batch 575 (50 docs)...\n",
      "Processing batch 576 (50 docs)...\n",
      "Processing batch 577 (50 docs)...\n",
      "Processing batch 578 (50 docs)...\n",
      "Processing batch 579 (50 docs)...\n",
      "Processing batch 580 (50 docs)...\n",
      "Processing batch 581 (50 docs)...\n",
      "Processing batch 582 (50 docs)...\n",
      "Processing batch 583 (50 docs)...\n",
      "Processing batch 584 (50 docs)...\n",
      "Processing batch 585 (50 docs)...\n",
      "Processing batch 586 (50 docs)...\n",
      "Processing batch 587 (50 docs)...\n",
      "Processing batch 588 (50 docs)...\n",
      "Processing batch 589 (50 docs)...\n",
      "Processing batch 590 (50 docs)...\n",
      "Processing batch 591 (50 docs)...\n",
      "Processing batch 592 (50 docs)...\n",
      "Processing batch 593 (50 docs)...\n",
      "Processing batch 594 (50 docs)...\n",
      "Processing batch 595 (50 docs)...\n",
      "Processing batch 596 (50 docs)...\n",
      "Processing batch 597 (50 docs)...\n",
      "Processing batch 598 (50 docs)...\n",
      "Processing batch 599 (50 docs)...\n",
      "Processing batch 600 (50 docs)...\n",
      "Processing batch 601 (50 docs)...\n",
      "Processing batch 602 (50 docs)...\n",
      "Processing batch 603 (50 docs)...\n",
      "Processing batch 604 (50 docs)...\n",
      "Processing batch 605 (50 docs)...\n",
      "Processing batch 606 (50 docs)...\n",
      "Processing batch 607 (50 docs)...\n",
      "Processing batch 608 (50 docs)...\n",
      "Processing batch 609 (50 docs)...\n",
      "Processing batch 610 (50 docs)...\n",
      "Processing batch 611 (50 docs)...\n",
      "Processing batch 612 (50 docs)...\n",
      "Processing batch 613 (50 docs)...\n",
      "Processing batch 614 (50 docs)...\n",
      "Processing batch 615 (50 docs)...\n",
      "Processing batch 616 (50 docs)...\n",
      "Processing batch 617 (50 docs)...\n",
      "Processing batch 618 (50 docs)...\n",
      "Processing batch 619 (50 docs)...\n",
      "Processing batch 620 (50 docs)...\n",
      "Processing batch 621 (50 docs)...\n",
      "Processing batch 622 (50 docs)...\n",
      "Processing batch 623 (50 docs)...\n",
      "Processing batch 624 (50 docs)...\n",
      "Processing batch 625 (50 docs)...\n",
      "Processing batch 626 (50 docs)...\n",
      "Processing batch 627 (50 docs)...\n",
      "Processing batch 628 (50 docs)...\n",
      "Processing batch 629 (50 docs)...\n",
      "Processing batch 630 (50 docs)...\n",
      "Processing batch 631 (50 docs)...\n",
      "Processing batch 632 (50 docs)...\n",
      "Processing batch 633 (50 docs)...\n",
      "Processing batch 634 (50 docs)...\n",
      "Processing batch 635 (50 docs)...\n",
      "Processing batch 636 (50 docs)...\n",
      "Processing batch 637 (50 docs)...\n",
      "Processing batch 638 (50 docs)...\n",
      "Processing batch 639 (50 docs)...\n",
      "Processing batch 640 (50 docs)...\n",
      "Processing batch 641 (50 docs)...\n",
      "Processing batch 642 (50 docs)...\n",
      "Processing batch 643 (50 docs)...\n",
      "Processing batch 644 (50 docs)...\n",
      "Processing batch 645 (50 docs)...\n",
      "Processing batch 646 (50 docs)...\n",
      "Processing batch 647 (50 docs)...\n",
      "Processing batch 648 (50 docs)...\n",
      "Processing batch 649 (50 docs)...\n",
      "Processing batch 650 (50 docs)...\n",
      "Processing batch 651 (50 docs)...\n",
      "Processing batch 652 (50 docs)...\n",
      "Processing batch 653 (50 docs)...\n",
      "Processing batch 654 (50 docs)...\n",
      "Processing batch 655 (50 docs)...\n",
      "Processing batch 656 (50 docs)...\n",
      "Processing batch 657 (50 docs)...\n",
      "Processing batch 658 (50 docs)...\n",
      "Processing batch 659 (50 docs)...\n",
      "Processing batch 660 (50 docs)...\n",
      "Processing batch 661 (50 docs)...\n",
      "Processing batch 662 (50 docs)...\n",
      "Processing batch 663 (50 docs)...\n",
      "Processing batch 664 (50 docs)...\n",
      "Processing batch 665 (50 docs)...\n",
      "Processing batch 666 (50 docs)...\n",
      "Processing batch 667 (50 docs)...\n",
      "Processing batch 668 (50 docs)...\n",
      "Processing batch 669 (50 docs)...\n",
      "Processing batch 670 (50 docs)...\n",
      "Processing batch 671 (50 docs)...\n",
      "Processing batch 672 (50 docs)...\n",
      "Processing batch 673 (50 docs)...\n",
      "Processing batch 674 (50 docs)...\n",
      "Processing batch 675 (50 docs)...\n",
      "Processing batch 676 (50 docs)...\n",
      "Processing batch 677 (50 docs)...\n",
      "Processing batch 678 (50 docs)...\n",
      "Processing batch 679 (50 docs)...\n",
      "Processing batch 680 (50 docs)...\n",
      "Processing batch 681 (50 docs)...\n",
      "Processing batch 682 (50 docs)...\n",
      "Processing batch 683 (50 docs)...\n",
      "Processing batch 684 (50 docs)...\n",
      "Processing batch 685 (50 docs)...\n",
      "Processing batch 686 (50 docs)...\n",
      "Processing batch 687 (50 docs)...\n",
      "Processing batch 688 (50 docs)...\n",
      "Processing batch 689 (50 docs)...\n",
      "Processing batch 690 (50 docs)...\n",
      "Processing batch 691 (50 docs)...\n",
      "Processing batch 692 (50 docs)...\n",
      "Processing batch 693 (50 docs)...\n",
      "Processing batch 694 (50 docs)...\n",
      "Processing batch 695 (50 docs)...\n",
      "Processing batch 696 (50 docs)...\n",
      "Processing batch 697 (50 docs)...\n",
      "Processing batch 698 (50 docs)...\n",
      "Processing batch 699 (50 docs)...\n",
      "Processing batch 700 (50 docs)...\n",
      "Processing batch 701 (50 docs)...\n",
      "Processing batch 702 (50 docs)...\n",
      "Processing batch 703 (50 docs)...\n",
      "Processing batch 704 (50 docs)...\n",
      "Processing batch 705 (50 docs)...\n",
      "Processing batch 706 (50 docs)...\n",
      "Processing batch 707 (50 docs)...\n",
      "Processing batch 708 (50 docs)...\n",
      "Processing batch 709 (50 docs)...\n",
      "Processing batch 710 (50 docs)...\n",
      "Processing batch 711 (50 docs)...\n",
      "Processing batch 712 (50 docs)...\n",
      "Processing batch 713 (50 docs)...\n",
      "Processing batch 714 (50 docs)...\n",
      "Processing batch 715 (50 docs)...\n",
      "Processing batch 716 (50 docs)...\n",
      "Processing batch 717 (50 docs)...\n",
      "Processing batch 718 (50 docs)...\n",
      "Processing batch 719 (50 docs)...\n",
      "Processing batch 720 (50 docs)...\n",
      "Processing batch 721 (50 docs)...\n",
      "Processing batch 722 (50 docs)...\n",
      "Processing batch 723 (50 docs)...\n",
      "Processing batch 724 (50 docs)...\n",
      "Processing batch 725 (50 docs)...\n",
      "Processing batch 726 (50 docs)...\n",
      "Processing batch 727 (50 docs)...\n",
      "Processing batch 728 (50 docs)...\n",
      "Processing batch 729 (50 docs)...\n",
      "Processing batch 730 (50 docs)...\n",
      "Processing batch 731 (50 docs)...\n",
      "Processing batch 732 (50 docs)...\n",
      "Processing batch 733 (50 docs)...\n",
      "Processing batch 734 (50 docs)...\n",
      "Processing batch 735 (50 docs)...\n",
      "Processing batch 736 (50 docs)...\n",
      "Processing batch 737 (50 docs)...\n",
      "Processing batch 738 (50 docs)...\n",
      "Processing batch 739 (50 docs)...\n",
      "Processing batch 740 (50 docs)...\n",
      "Processing batch 741 (50 docs)...\n",
      "Processing batch 742 (50 docs)...\n",
      "Processing batch 743 (50 docs)...\n",
      "Processing batch 744 (50 docs)...\n",
      "Processing batch 745 (50 docs)...\n",
      "Processing batch 746 (50 docs)...\n",
      "Processing batch 747 (50 docs)...\n",
      "Processing batch 748 (50 docs)...\n",
      "Processing batch 749 (50 docs)...\n",
      "Processing batch 750 (50 docs)...\n",
      "Processing batch 751 (50 docs)...\n",
      "Processing batch 752 (50 docs)...\n",
      "Processing batch 753 (50 docs)...\n",
      "Processing batch 754 (50 docs)...\n",
      "Processing batch 755 (50 docs)...\n",
      "Processing batch 756 (50 docs)...\n",
      "Processing batch 757 (50 docs)...\n",
      "Processing batch 758 (50 docs)...\n",
      "Processing batch 759 (50 docs)...\n",
      "Processing batch 760 (50 docs)...\n",
      "Processing batch 761 (50 docs)...\n",
      "Processing batch 762 (50 docs)...\n",
      "Processing batch 763 (50 docs)...\n",
      "Processing batch 764 (50 docs)...\n",
      "Processing batch 765 (50 docs)...\n",
      "Processing batch 766 (50 docs)...\n",
      "Processing batch 767 (50 docs)...\n",
      "Processing batch 768 (50 docs)...\n",
      "Processing batch 769 (50 docs)...\n",
      "Processing batch 770 (50 docs)...\n",
      "Processing batch 771 (50 docs)...\n",
      "Processing batch 772 (50 docs)...\n",
      "Processing batch 773 (50 docs)...\n",
      "Processing batch 774 (50 docs)...\n",
      "Processing batch 775 (50 docs)...\n",
      "Processing batch 776 (50 docs)...\n",
      "Processing batch 777 (50 docs)...\n",
      "Processing batch 778 (50 docs)...\n",
      "Processing batch 779 (50 docs)...\n",
      "Processing batch 780 (50 docs)...\n",
      "Processing batch 781 (50 docs)...\n",
      "Processing batch 782 (50 docs)...\n",
      "Processing batch 783 (50 docs)...\n",
      "Processing batch 784 (50 docs)...\n",
      "Processing batch 785 (50 docs)...\n",
      "Processing batch 786 (50 docs)...\n",
      "Processing batch 787 (50 docs)...\n",
      "Processing batch 788 (50 docs)...\n",
      "Processing batch 789 (50 docs)...\n",
      "Processing batch 790 (50 docs)...\n",
      "Processing batch 791 (50 docs)...\n",
      "Processing batch 792 (50 docs)...\n",
      "Processing batch 793 (50 docs)...\n",
      "Processing batch 794 (50 docs)...\n",
      "Processing batch 795 (50 docs)...\n",
      "Processing batch 796 (50 docs)...\n",
      "Processing batch 797 (50 docs)...\n",
      "Processing batch 798 (50 docs)...\n",
      "Processing batch 799 (50 docs)...\n",
      "Processing batch 800 (50 docs)...\n",
      "Processing batch 801 (50 docs)...\n",
      "Processing batch 802 (41 docs)...\n",
      "Structured dataset saved to structured_input_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "DATA_DIR = r\"C:\\Users\\islam hitham\\paper_extractor_dataset\\json\"\n",
    "OUTPUT_FILE = \"structured_input_output.jsonl\"\n",
    "BATCH_SIZE = 50  # optional, for processing in batches\n",
    "\n",
    "# ---------------- LOAD & CONVERT ----------------\n",
    "def convert_document(doc):\n",
    "    # Get full text (abstract + body)\n",
    "    full_text = \"\"\n",
    "    # Abstract sentences\n",
    "    abstract_texts = []\n",
    "    for s in doc.get(\"abstract\", []):\n",
    "        if isinstance(s, dict):\n",
    "            abstract_texts.append(s.get(\"text\", \"\"))\n",
    "        else:\n",
    "            abstract_texts.append(str(s))\n",
    "    # Body sentences\n",
    "    body_texts = []\n",
    "    for b in doc.get(\"body_text\", []):\n",
    "        if isinstance(b, dict):\n",
    "            body_texts.append(b.get(\"text\", \"\"))\n",
    "        else:\n",
    "            body_texts.append(str(b))\n",
    "    full_text = \" \".join(abstract_texts + body_texts)\n",
    "\n",
    "    # Everything else is output\n",
    "    output = {k: v for k, v in doc.items() if k != \"abstract\" and k != \"body_text\"}\n",
    "    output[\"abstract\"] = doc.get(\"abstract\", [])\n",
    "    output[\"body_text\"] = doc.get(\"body_text\", [])\n",
    "\n",
    "    return {\"input\": full_text, \"output\": output}\n",
    "\n",
    "def process_all_documents(data_dir, output_file, batch_size=50):\n",
    "    json_files = glob(os.path.join(data_dir, \"**\", \"*.json\"), recursive=True)\n",
    "    print(f\"Found {len(json_files)} JSON files.\")\n",
    "\n",
    "    if len(json_files) == 0:\n",
    "        print(\"No JSON files found. Check the path.\")\n",
    "        return\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for i in range(0, len(json_files), batch_size):\n",
    "            batch_files = json_files[i:i + batch_size]\n",
    "            print(f\"Processing batch {i//batch_size + 1} ({len(batch_files)} docs)...\")\n",
    "            for file_path in batch_files:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    try:\n",
    "                        raw_doc = json.load(f)\n",
    "                        item = convert_document(raw_doc)\n",
    "                        out_f.write(json.dumps(item) + \"\\n\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    print(f\"Structured dataset saved to {output_file}\")\n",
    "\n",
    "# ---------------- RUN ----------------\n",
    "process_all_documents(DATA_DIR, OUTPUT_FILE, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "388d4c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 1 ===\n",
      "\n",
      "--- Input (Full Text) ---\n",
      "O b j e c t i v e   T h e   o v e r a l l   r e s e a r c h   o b j e c t i v e   w a s   t o   t h e o r e t i c a l l y   a n d   e m p i r i c a l l y   d e v e l o p   t h e   i d e a s   a r o u n d   a   s y s t e m   o f   s a f e t y   m a n a g e m e n t   p r a c t i c e s   ( t e n   p r a c t i c e s   w e r e   e l a b o r a t e d ) ,   t o   t e s t   t h e i r   r e l a t i o n s h i p   w i t h   o b j e c t i v e   s a f e t y   s t a t i s t i c s   ( s u c h   a s   a c c i d e n t   r a t e s ) ,   a n d   t o   e x p l o r e   h o w   t h e s e   p r a c t i c e s   w o r k   t o   a c h i e v e   p o s i t i v e   s a f e t y   r e s u l t s   ( a c c i d e n t   p r e v e n t i o n )   t h r o u g h   w o r k e r   e n g a g e m e n t .   M e t h o d   D a t a   w e r e   c o l l e c t e d   u s i n g   s a f e t y   m a n a g e r ,   s u p e r v i s o r   a n d   e m p l o y e e   s u r v e y s   d e s i g n e d   t o   a s s e s s   a n d   l i n k   s a f e t ...\n",
      "\n",
      "--- Output Keys ---\n",
      "['author_highlights', 'bib_entries', 'docId', 'metadata', 'abstract', 'body_text']\n",
      "\n",
      "--- Metadata Sample ---\n",
      "asjc: ['2213', '2739', '3307']\n",
      "authors: [{'email': 'jan.wachter@iup.edu', 'first': 'Jan K.', 'initial': 'J.K.', 'last': 'Wachter'}, {'email': None, 'first': 'Patrick L.', 'initial': 'P.L.', 'last': 'Yorio'}]\n",
      "doi: 10.1016/j.aap.2013.07.029\n",
      "firstpage: 117\n",
      "issn: 00014575\n",
      "keywords: ['Accident prevention', 'Accident rates', 'Human performance', 'Safety management systems', 'Worker engagement']\n",
      "lastpage: 130\n",
      "openaccess: Full\n",
      "pub_year: 2014\n",
      "subjareas: ['ENGI', 'MEDI', 'SOCI']\n",
      "title: A system of safety management practices and worker engagement for reducing and preventing accidents: An empirical and theoretical investigation\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Example 2 ===\n",
      "\n",
      "--- Input (Full Text) ---\n",
      "T h e   o b j e c t i v e   o f   a n   a c c i d e n t - m a p p i n g   a l g o r i t h m   i s   t o   s n a p   t r a f f i c   a c c i d e n t s   o n t o   t h e   c o r r e c t   r o a d   s e g m e n t s .   A s s i g n i n g   a c c i d e n t s   o n t o   t h e   c o r r e c t   s e g m e n t s   f a c i l i t a t e   t o   r o b u s t l y   c a r r y   o u t   s o m e   k e y   a n a l y s e s   i n   a c c i d e n t   r e s e a r c h   i n c l u d i n g   t h e   i d e n t i f i c a t i o n   o f   a c c i d e n t   h o t - s p o t s ,   n e t w o r k - l e v e l   r i s k   m a p p i n g   a n d   s e g m e n t - l e v e l   a c c i d e n t   r i s k   m o d e l l i n g .   E x i s t i n g   r i s k   m a p p i n g   a l g o r i t h m s   h a v e   s o m e   s e v e r e   l i m i t a t i o n s :   ( i )   t h e y   a r e   n o t   e a s i l y   ' t r a n s f e r a b l e '   a s   t h e   a l g o r i t h m s   a r e   s p e c i f i c   t o   g i v e n   a c c i d e n t   d ...\n",
      "\n",
      "--- Output Keys ---\n",
      "['author_highlights', 'bib_entries', 'docId', 'metadata', 'abstract', 'body_text']\n",
      "\n",
      "--- Metadata Sample ---\n",
      "asjc: ['2213', '2739', '3307']\n",
      "authors: [{'email': 'l.deka@lboro.ac.uk', 'first': 'Lipika', 'initial': 'L.', 'last': 'Deka'}, {'email': 'm.a.quddus@lboro.ac.uk', 'first': 'Mohammed', 'initial': 'M.', 'last': 'Quddus'}]\n",
      "doi: 10.1016/j.aap.2013.12.001\n",
      "firstpage: 105\n",
      "issn: 00014575\n",
      "keywords: ['Accident-mapping', 'Artificial neural network', 'Pattern-matching']\n",
      "lastpage: 113\n",
      "openaccess: Full\n",
      "pub_year: 2014\n",
      "subjareas: ['ENGI', 'MEDI', 'SOCI']\n",
      "title: Network-level accident-mapping: Distance based pattern matching using artificial neural network\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "FILE_PATH = \"structured_input_output.jsonl\"\n",
    "\n",
    "# ---------------- SHOW FIRST 2 EXAMPLES ----------------\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(2):\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        doc = json.loads(line)\n",
    "        print(f\"=== Example {i+1} ===\")\n",
    "        print(\"\\n--- Input (Full Text) ---\")\n",
    "        print(doc[\"input\"][:1000] + \"...\" if len(doc[\"input\"]) > 500 else doc[\"input\"])  # first 500 chars\n",
    "        print(\"\\n--- Output Keys ---\")\n",
    "        print(list(doc[\"output\"].keys()))\n",
    "        print(\"\\n--- Metadata Sample ---\")\n",
    "        for k, v in doc[\"output\"].get(\"metadata\", {}).items():\n",
    "            print(f\"{k}: {v}\")\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8577f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed full text for 40091 documents. Saved to structured_input_output_fixed_1.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re \n",
    "\n",
    "INPUT_FILE = \"structured_input_output.jsonl\"\n",
    "OUTPUT_FILE = \"structured_input_output_fixed_1.jsonl\"\n",
    "\n",
    "fixed_count = 0\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as in_f, \\\n",
    "     open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as out_f:\n",
    "\n",
    "    for line in in_f:\n",
    "        try:\n",
    "            doc = json.loads(line)\n",
    "            \n",
    "            # Rebuild full text from abstract + body_text\n",
    "            abstract_texts = []\n",
    "            for s in doc[\"output\"].get(\"abstract\", []):\n",
    "                if isinstance(s, dict):\n",
    "                    abstract_texts.append(s.get(\"text\", \"\"))\n",
    "                else:\n",
    "                    abstract_texts.append(str(s))\n",
    "\n",
    "            body_texts = []\n",
    "            for b in doc[\"output\"].get(\"body_text\", []):\n",
    "                if isinstance(b, dict):\n",
    "                    body_texts.append(b.get(\"text\", \"\"))\n",
    "                else:\n",
    "                    body_texts.append(str(b))\n",
    "\n",
    "            # Join the text fragments; they still contain the bad spacing.\n",
    "            full_text = \" \".join(abstract_texts + body_texts)\n",
    "\n",
    "            # --- START FIXING THE SPACING ---\n",
    "            \n",
    "            # 1. Replace non-breaking spaces ('\\xa0') with standard spaces.\n",
    "            cleaned_text = full_text.replace('\\xa0', ' ')\n",
    "\n",
    "            # 2. Use regex to find sequences of two or more spaces (which separate words) \n",
    "            # and replace them with a unique temporary separator ('|').\n",
    "            # This isolates the words, which still have single internal spaces (e.g., 'O b j e c t i v e').\n",
    "            text_with_separators = re.sub(r'\\s{2,}', '|', cleaned_text).strip()\n",
    "\n",
    "            # 3. Split the text by the separator.\n",
    "            spaced_words = text_with_separators.split('|')\n",
    "            \n",
    "            # 4. For each resulting fragment, remove all remaining internal single spaces (' '), \n",
    "            # effectively collapsing 'O b j e c t i v e' into 'Objective'.\n",
    "            clean_words = [word.replace(' ', '').strip() for word in spaced_words if word.strip()]\n",
    "            \n",
    "            # 5. Join the fully clean words with a single, correct space.\n",
    "            final_full_text = ' '.join(clean_words)\n",
    "            \n",
    "            # --- END FIXING THE SPACING ---\n",
    "            \n",
    "            # Replace input with correctly fixed text\n",
    "            doc[\"input\"] = final_full_text\n",
    "\n",
    "            out_f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "            fixed_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fixing line: {e}\")\n",
    "\n",
    "print(f\"Fixed full text for {fixed_count} documents. Saved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3df61190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 1 ===\n",
      "\n",
      "--- Input (Full Text) ---\n",
      "Objective The overall research objective was to theoretically and empirically develop the ideas around a system of safety management practices (ten practices were elaborated), to test their relationship with objective safety statistics (such as accident rates), and to explore how these practices work to achieve positive safety results (accident prevention) through worker engagement. Method Data were collected using safety manager, supervisor and employee surveys designed to assess and link safety management system practices, employee perceptions resulting from existing practices, and safety performance outcomes. Results Results indicate the following: there is a significant negative relationship between the presence of ten individual safety management practices, as well as the composite of these practices, with accident rates; there is a significant negative relationship between the level of safety-focused worker emotional and cognitive engagement with accident rates; safety management...\n",
      "\n",
      "--- Output Keys ---\n",
      "['author_highlights', 'bib_entries', 'docId', 'metadata', 'abstract', 'body_text']\n",
      "\n",
      "--- Metadata Sample ---\n",
      "asjc: ['2213', '2739', '3307']\n",
      "authors: [{'email': 'jan.wachter@iup.edu', 'first': 'Jan K.', 'initial': 'J.K.', 'last': 'Wachter'}, {'email': None, 'first': 'Patrick L.', 'initial': 'P.L.', 'last': 'Yorio'}]\n",
      "doi: 10.1016/j.aap.2013.07.029\n",
      "firstpage: 117\n",
      "issn: 00014575\n",
      "keywords: ['Accident prevention', 'Accident rates', 'Human performance', 'Safety management systems', 'Worker engagement']\n",
      "lastpage: 130\n",
      "openaccess: Full\n",
      "pub_year: 2014\n",
      "subjareas: ['ENGI', 'MEDI', 'SOCI']\n",
      "title: A system of safety management practices and worker engagement for reducing and preventing accidents: An empirical and theoretical investigation\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Example 2 ===\n",
      "\n",
      "--- Input (Full Text) ---\n",
      "The objective of an accident-mapping algorithm is to snap traffic accidents onto the correct road segments. Assigning accidents onto the correct segments facilitate to robustly carry out some key analyses in accident research including the identification of accident hot-spots, network-level risk mapping and segment-level accident risk modelling. Existing risk mapping algorithms have some severe limitations: (i) they are not easily 'transferable' as the algorithms are specific to given accident datasets; (ii) they do not perform well in all road-network environments such as in areas of dense road network; and (iii) the methods used do not perform well in addressing inaccuracies inherent in and type of road environment. The purpose of this paper is to develop a new accident mapping algorithm based on the common variables observed in most accident databases (e.g. road name and type, direction of vehicle movement before the accident and recorded accident location). The challenges here are ...\n",
      "\n",
      "--- Output Keys ---\n",
      "['author_highlights', 'bib_entries', 'docId', 'metadata', 'abstract', 'body_text']\n",
      "\n",
      "--- Metadata Sample ---\n",
      "asjc: ['2213', '2739', '3307']\n",
      "authors: [{'email': 'l.deka@lboro.ac.uk', 'first': 'Lipika', 'initial': 'L.', 'last': 'Deka'}, {'email': 'm.a.quddus@lboro.ac.uk', 'first': 'Mohammed', 'initial': 'M.', 'last': 'Quddus'}]\n",
      "doi: 10.1016/j.aap.2013.12.001\n",
      "firstpage: 105\n",
      "issn: 00014575\n",
      "keywords: ['Accident-mapping', 'Artificial neural network', 'Pattern-matching']\n",
      "lastpage: 113\n",
      "openaccess: Full\n",
      "pub_year: 2014\n",
      "subjareas: ['ENGI', 'MEDI', 'SOCI']\n",
      "title: Network-level accident-mapping: Distance based pattern matching using artificial neural network\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "FILE_PATH = \"structured_input_output_fixed_1.jsonl\"\n",
    "\n",
    "# ---------------- SHOW FIRST 2 EXAMPLES ----------------\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(2):\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        doc = json.loads(line)\n",
    "        print(f\"=== Example {i+1} ===\")\n",
    "        print(\"\\n--- Input (Full Text) ---\")\n",
    "        print(doc[\"input\"][:1000] + \"...\" if len(doc[\"input\"]) > 500 else doc[\"input\"])  # first 500 chars\n",
    "        print(\"\\n--- Output Keys ---\")\n",
    "        print(list(doc[\"output\"].keys()))\n",
    "        print(\"\\n--- Metadata Sample ---\")\n",
    "        for k, v in doc[\"output\"].get(\"metadata\", {}).items():\n",
    "            print(f\"{k}: {v}\")\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4938c11",
   "metadata": {},
   "source": [
    "### formatter and train test valid split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "348ea5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from C:\\Users\\islam hitham\\Paper-LLM-Extractor\\notebooks\\structured_input_output_fixed_1.jsonl...\n",
      "Skipping line 102: Empty 'input' field.\n",
      "Skipping line 1144: Empty 'input' field.\n",
      "Skipping line 1257: Empty 'input' field.\n",
      "Skipping line 1605: Empty 'input' field.\n",
      "Skipping line 1747: Empty 'input' field.\n",
      "Skipping line 2440: Empty 'input' field.\n",
      "Skipping line 2445: Empty 'input' field.\n",
      "Skipping line 2500: Empty 'input' field.\n",
      "Skipping line 2597: Empty 'input' field.\n",
      "Skipping line 2956: Empty 'input' field.\n",
      "Skipping line 3258: Empty 'input' field.\n",
      "Skipping line 3440: Empty 'input' field.\n",
      "Skipping line 3724: Empty 'input' field.\n",
      "Skipping line 4457: Empty 'input' field.\n",
      "Skipping line 4726: Empty 'input' field.\n",
      "Skipping line 4828: Empty 'input' field.\n",
      "Skipping line 4846: Empty 'input' field.\n",
      "Skipping line 4889: Empty 'input' field.\n",
      "Skipping line 4896: Empty 'input' field.\n",
      "Skipping line 5308: Empty 'input' field.\n",
      "Skipping line 6124: Empty 'input' field.\n",
      "Skipping line 6125: Empty 'input' field.\n",
      "Skipping line 6504: Empty 'input' field.\n",
      "Skipping line 6928: Empty 'input' field.\n",
      "Skipping line 6953: Empty 'input' field.\n",
      "Skipping line 7198: Empty 'input' field.\n",
      "Skipping line 7673: Empty 'input' field.\n",
      "Skipping line 7681: Empty 'input' field.\n",
      "Skipping line 7682: Empty 'input' field.\n",
      "Skipping line 7685: Empty 'input' field.\n",
      "Skipping line 7690: Empty 'input' field.\n",
      "Skipping line 7692: Empty 'input' field.\n",
      "Skipping line 7763: Empty 'input' field.\n",
      "Skipping line 8252: Empty 'input' field.\n",
      "Skipping line 8348: Empty 'input' field.\n",
      "Skipping line 9331: Empty 'input' field.\n",
      "Skipping line 9675: Empty 'input' field.\n",
      "Skipping line 9728: Empty 'input' field.\n",
      "Skipping line 10030: Empty 'input' field.\n",
      "Skipping line 10047: Empty 'input' field.\n",
      "Skipping line 10296: Empty 'input' field.\n",
      "Skipping line 10298: Empty 'input' field.\n",
      "Skipping line 10301: Empty 'input' field.\n",
      "Skipping line 10940: Empty 'input' field.\n",
      "Skipping line 10982: Empty 'input' field.\n",
      "Skipping line 11002: Empty 'input' field.\n",
      "Skipping line 11094: Empty 'input' field.\n",
      "Skipping line 11165: Empty 'input' field.\n",
      "Skipping line 11293: Empty 'input' field.\n",
      "Skipping line 11696: Empty 'input' field.\n",
      "Skipping line 11744: Empty 'input' field.\n",
      "Skipping line 11814: Empty 'input' field.\n",
      "Skipping line 11875: Empty 'input' field.\n",
      "Skipping line 11876: Empty 'input' field.\n",
      "Skipping line 11878: Empty 'input' field.\n",
      "Skipping line 11879: Empty 'input' field.\n",
      "Skipping line 11880: Empty 'input' field.\n",
      "Skipping line 11883: Empty 'input' field.\n",
      "Skipping line 11884: Empty 'input' field.\n",
      "Skipping line 11890: Empty 'input' field.\n",
      "Skipping line 11907: Empty 'input' field.\n",
      "Skipping line 11959: Empty 'input' field.\n",
      "Skipping line 11985: Empty 'input' field.\n",
      "Skipping line 12001: Empty 'input' field.\n",
      "Skipping line 12012: Empty 'input' field.\n",
      "Skipping line 12015: Empty 'input' field.\n",
      "Skipping line 12016: Empty 'input' field.\n",
      "Skipping line 12017: Empty 'input' field.\n",
      "Skipping line 12018: Empty 'input' field.\n",
      "Skipping line 12019: Empty 'input' field.\n",
      "Skipping line 12020: Empty 'input' field.\n",
      "Skipping line 12021: Empty 'input' field.\n",
      "Skipping line 12022: Empty 'input' field.\n",
      "Skipping line 12023: Empty 'input' field.\n",
      "Skipping line 12024: Empty 'input' field.\n",
      "Skipping line 12030: Empty 'input' field.\n",
      "Skipping line 12222: Empty 'input' field.\n",
      "Skipping line 12230: Empty 'input' field.\n",
      "Skipping line 12355: Empty 'input' field.\n",
      "Skipping line 12356: Empty 'input' field.\n",
      "Skipping line 12357: Empty 'input' field.\n",
      "Skipping line 12374: Empty 'input' field.\n",
      "Skipping line 12435: Empty 'input' field.\n",
      "Skipping line 12644: Empty 'input' field.\n",
      "Skipping line 12648: Empty 'input' field.\n",
      "Skipping line 12906: Empty 'input' field.\n",
      "Skipping line 12907: Empty 'input' field.\n",
      "Skipping line 13132: Empty 'input' field.\n",
      "Skipping line 13304: Empty 'input' field.\n",
      "Skipping line 13313: Empty 'input' field.\n",
      "Skipping line 13314: Empty 'input' field.\n",
      "Skipping line 13661: Empty 'input' field.\n",
      "Skipping line 14061: Empty 'input' field.\n",
      "Skipping line 14653: Empty 'input' field.\n",
      "Skipping line 14723: Empty 'input' field.\n",
      "Skipping line 14850: Empty 'input' field.\n",
      "Skipping line 15487: Empty 'input' field.\n",
      "Skipping line 15653: Empty 'input' field.\n",
      "Skipping line 15938: Empty 'input' field.\n",
      "Skipping line 16171: Empty 'input' field.\n",
      "Skipping line 16363: Empty 'input' field.\n",
      "Skipping line 16942: Empty 'input' field.\n",
      "Skipping line 16966: Empty 'input' field.\n",
      "Skipping line 17358: Empty 'input' field.\n",
      "Skipping line 17408: Empty 'input' field.\n",
      "Skipping line 17428: Empty 'input' field.\n",
      "Skipping line 17429: Empty 'input' field.\n",
      "Skipping line 17439: Empty 'input' field.\n",
      "Skipping line 17440: Empty 'input' field.\n",
      "Skipping line 17464: Empty 'input' field.\n",
      "Skipping line 17468: Empty 'input' field.\n",
      "Skipping line 17481: Empty 'input' field.\n",
      "Skipping line 17615: Empty 'input' field.\n",
      "Skipping line 17893: Empty 'input' field.\n",
      "Skipping line 18098: Empty 'input' field.\n",
      "Skipping line 18105: Empty 'input' field.\n",
      "Skipping line 18713: Empty 'input' field.\n",
      "Skipping line 18789: Empty 'input' field.\n",
      "Skipping line 19841: Empty 'input' field.\n",
      "Skipping line 20109: Empty 'input' field.\n",
      "Skipping line 20386: Empty 'input' field.\n",
      "Skipping line 20391: Empty 'input' field.\n",
      "Skipping line 20502: Empty 'input' field.\n",
      "Skipping line 20555: Empty 'input' field.\n",
      "Skipping line 20844: Empty 'input' field.\n",
      "Skipping line 20857: Empty 'input' field.\n",
      "Skipping line 20972: Empty 'input' field.\n",
      "Skipping line 21301: Empty 'input' field.\n",
      "Skipping line 21380: Empty 'input' field.\n",
      "Skipping line 21385: Empty 'input' field.\n",
      "Skipping line 21490: Empty 'input' field.\n",
      "Skipping line 21504: Empty 'input' field.\n",
      "Skipping line 21507: Empty 'input' field.\n",
      "Skipping line 21695: Empty 'input' field.\n",
      "Skipping line 21760: Empty 'input' field.\n",
      "Skipping line 21982: Empty 'input' field.\n",
      "Skipping line 22244: Empty 'input' field.\n",
      "Skipping line 22265: Empty 'input' field.\n",
      "Skipping line 22278: Empty 'input' field.\n",
      "Skipping line 22291: Empty 'input' field.\n",
      "Skipping line 22300: Empty 'input' field.\n",
      "Skipping line 22834: Empty 'input' field.\n",
      "Skipping line 22848: Empty 'input' field.\n",
      "Skipping line 22943: Empty 'input' field.\n",
      "Skipping line 23031: Empty 'input' field.\n",
      "Skipping line 23357: Empty 'input' field.\n",
      "Skipping line 23652: Empty 'input' field.\n",
      "Skipping line 23965: Empty 'input' field.\n",
      "Skipping line 24789: Empty 'input' field.\n",
      "Skipping line 24823: Empty 'input' field.\n",
      "Skipping line 24923: Empty 'input' field.\n",
      "Skipping line 24982: Empty 'input' field.\n",
      "Skipping line 25679: Empty 'input' field.\n",
      "Skipping line 25727: Empty 'input' field.\n",
      "Skipping line 25838: Empty 'input' field.\n",
      "Skipping line 26265: Empty 'input' field.\n",
      "Skipping line 26267: Empty 'input' field.\n",
      "Skipping line 26615: Empty 'input' field.\n",
      "Skipping line 26642: Empty 'input' field.\n",
      "Skipping line 26661: Empty 'input' field.\n",
      "Skipping line 26674: Empty 'input' field.\n",
      "Skipping line 26682: Empty 'input' field.\n",
      "Skipping line 26683: Empty 'input' field.\n",
      "Skipping line 26686: Empty 'input' field.\n",
      "Skipping line 26690: Empty 'input' field.\n",
      "Skipping line 26697: Empty 'input' field.\n",
      "Skipping line 26710: Empty 'input' field.\n",
      "Skipping line 26717: Empty 'input' field.\n",
      "Skipping line 26738: Empty 'input' field.\n",
      "Skipping line 26783: Empty 'input' field.\n",
      "Skipping line 26787: Empty 'input' field.\n",
      "Skipping line 26797: Empty 'input' field.\n",
      "Skipping line 26803: Empty 'input' field.\n",
      "Skipping line 26821: Empty 'input' field.\n",
      "Skipping line 26824: Empty 'input' field.\n",
      "Skipping line 26829: Empty 'input' field.\n",
      "Skipping line 26831: Empty 'input' field.\n",
      "Skipping line 27242: Empty 'input' field.\n",
      "Skipping line 27245: Empty 'input' field.\n",
      "Skipping line 27247: Empty 'input' field.\n",
      "Skipping line 27252: Empty 'input' field.\n",
      "Skipping line 27297: Empty 'input' field.\n",
      "Skipping line 27320: Empty 'input' field.\n",
      "Skipping line 27323: Empty 'input' field.\n",
      "Skipping line 27328: Empty 'input' field.\n",
      "Skipping line 27329: Empty 'input' field.\n",
      "Skipping line 27334: Empty 'input' field.\n",
      "Skipping line 27336: Empty 'input' field.\n",
      "Skipping line 27340: Empty 'input' field.\n",
      "Skipping line 27344: Empty 'input' field.\n",
      "Skipping line 27345: Empty 'input' field.\n",
      "Skipping line 27347: Empty 'input' field.\n",
      "Skipping line 27348: Empty 'input' field.\n",
      "Skipping line 27352: Empty 'input' field.\n",
      "Skipping line 27384: Empty 'input' field.\n",
      "Skipping line 27386: Empty 'input' field.\n",
      "Skipping line 27392: Empty 'input' field.\n",
      "Skipping line 27396: Empty 'input' field.\n",
      "Skipping line 27455: Empty 'input' field.\n",
      "Skipping line 27563: Empty 'input' field.\n",
      "Skipping line 27692: Empty 'input' field.\n",
      "Skipping line 27837: Empty 'input' field.\n",
      "Skipping line 27883: Empty 'input' field.\n",
      "Skipping line 27920: Empty 'input' field.\n",
      "Skipping line 28338: Empty 'input' field.\n",
      "Skipping line 28583: Empty 'input' field.\n",
      "Skipping line 28584: Empty 'input' field.\n",
      "Skipping line 28598: Empty 'input' field.\n",
      "Skipping line 28599: Empty 'input' field.\n",
      "Skipping line 28610: Empty 'input' field.\n",
      "Skipping line 28611: Empty 'input' field.\n",
      "Skipping line 28619: Empty 'input' field.\n",
      "Skipping line 28637: Empty 'input' field.\n",
      "Skipping line 28638: Empty 'input' field.\n",
      "Skipping line 28639: Empty 'input' field.\n",
      "Skipping line 28640: Empty 'input' field.\n",
      "Skipping line 28641: Empty 'input' field.\n",
      "Skipping line 28642: Empty 'input' field.\n",
      "Skipping line 28643: Empty 'input' field.\n",
      "Skipping line 28644: Empty 'input' field.\n",
      "Skipping line 28645: Empty 'input' field.\n",
      "Skipping line 28646: Empty 'input' field.\n",
      "Skipping line 28647: Empty 'input' field.\n",
      "Skipping line 28648: Empty 'input' field.\n",
      "Skipping line 28649: Empty 'input' field.\n",
      "Skipping line 28661: Empty 'input' field.\n",
      "Skipping line 28671: Empty 'input' field.\n",
      "Skipping line 28672: Empty 'input' field.\n",
      "Skipping line 28673: Empty 'input' field.\n",
      "Skipping line 28686: Empty 'input' field.\n",
      "Skipping line 28687: Empty 'input' field.\n",
      "Skipping line 28719: Empty 'input' field.\n",
      "Skipping line 28734: Empty 'input' field.\n",
      "Skipping line 28751: Empty 'input' field.\n",
      "Skipping line 28769: Empty 'input' field.\n",
      "Skipping line 28770: Empty 'input' field.\n",
      "Skipping line 28771: Empty 'input' field.\n",
      "Skipping line 28806: Empty 'input' field.\n",
      "Skipping line 28807: Empty 'input' field.\n",
      "Skipping line 28808: Empty 'input' field.\n",
      "Skipping line 28909: Empty 'input' field.\n",
      "Skipping line 28982: Empty 'input' field.\n",
      "Skipping line 30093: Empty 'input' field.\n",
      "Skipping line 30479: Empty 'input' field.\n",
      "Skipping line 30480: Empty 'input' field.\n",
      "Skipping line 30549: Empty 'input' field.\n",
      "Skipping line 30565: Empty 'input' field.\n",
      "Skipping line 30571: Empty 'input' field.\n",
      "Skipping line 30614: Empty 'input' field.\n",
      "Skipping line 30676: Empty 'input' field.\n",
      "Skipping line 30679: Empty 'input' field.\n",
      "Skipping line 30686: Empty 'input' field.\n",
      "Skipping line 30705: Empty 'input' field.\n",
      "Skipping line 30706: Empty 'input' field.\n",
      "Skipping line 30709: Empty 'input' field.\n",
      "Skipping line 30710: Empty 'input' field.\n",
      "Skipping line 30783: Empty 'input' field.\n",
      "Skipping line 30796: Empty 'input' field.\n",
      "Skipping line 30804: Empty 'input' field.\n",
      "Skipping line 30844: Empty 'input' field.\n",
      "Skipping line 30924: Empty 'input' field.\n",
      "Skipping line 31300: Empty 'input' field.\n",
      "Skipping line 31705: Empty 'input' field.\n",
      "Skipping line 33599: Empty 'input' field.\n",
      "Skipping line 33623: Empty 'input' field.\n",
      "Skipping line 33791: Empty 'input' field.\n",
      "Skipping line 33921: Empty 'input' field.\n",
      "Skipping line 36959: Empty 'input' field.\n",
      "Skipping line 36988: Empty 'input' field.\n",
      "Skipping line 37003: Empty 'input' field.\n",
      "Skipping line 37023: Empty 'input' field.\n",
      "Skipping line 37164: Empty 'input' field.\n",
      "Skipping line 37165: Empty 'input' field.\n",
      "Skipping line 37258: Empty 'input' field.\n",
      "Skipping line 37286: Empty 'input' field.\n",
      "Skipping line 37525: Empty 'input' field.\n",
      "Skipping line 37529: Empty 'input' field.\n",
      "Skipping line 37621: Empty 'input' field.\n",
      "Skipping line 37810: Empty 'input' field.\n",
      "Skipping line 37893: Empty 'input' field.\n",
      "Skipping line 38334: Empty 'input' field.\n",
      "Skipping line 38618: Empty 'input' field.\n",
      "Skipping line 38749: Empty 'input' field.\n",
      "Skipping line 38789: Empty 'input' field.\n",
      "Skipping line 39174: Empty 'input' field.\n",
      "Skipping line 39261: Empty 'input' field.\n",
      "Skipping line 39355: Empty 'input' field.\n",
      "Skipping line 39383: Empty 'input' field.\n",
      "Skipping line 39574: Empty 'input' field.\n",
      "Skipping line 39575: Empty 'input' field.\n",
      "Skipping line 39576: Empty 'input' field.\n",
      "Skipping line 39578: Empty 'input' field.\n",
      "Skipping line 39579: Empty 'input' field.\n",
      "Skipping line 39583: Empty 'input' field.\n",
      "Skipping line 39586: Empty 'input' field.\n",
      "Skipping line 39590: Empty 'input' field.\n",
      "Skipping line 39591: Empty 'input' field.\n",
      "Skipping line 39598: Empty 'input' field.\n",
      "Skipping line 39638: Empty 'input' field.\n",
      "Skipping line 39728: Empty 'input' field.\n",
      "Skipping line 39962: Empty 'input' field.\n",
      "Total documents loaded and formatted: 39790\n",
      "--- Dataset Split Summary ---\n",
      "Train set size: 31832\n",
      "Validation set size: 3979\n",
      "Test set size: 3979\n",
      "\n",
      "Data splitting and formatting complete!\n",
      "Files saved: llama_train.jsonl, llama_valid.jsonl, llama_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_FILE = r\"C:\\Users\\islam hitham\\Paper-LLM-Extractor\\notebooks\\structured_input_output_fixed_1.jsonl\"\n",
    "TRAIN_FILE = \"llama_train.jsonl\"\n",
    "VALID_FILE = \"llama_valid.jsonl\"\n",
    "TEST_FILE = \"llama_test.jsonl\"\n",
    "\n",
    "# Split percentages (80/10/10)\n",
    "TRAIN_RATIO = 0.8\n",
    "VALID_RATIO = 0.1\n",
    "# Test ratio will be the remainder (0.1)\n",
    "\n",
    "# --- Llama 3.2 Instruct Format Template ---\n",
    "# The model is trained to predict the assistant's response given the user's prompt.\n",
    "SYSTEM_INSTRUCTION = (\n",
    "    \"You are an expert academic document extractor. Your task is to process the full text of an \"\n",
    "    \"academic paper and accurately extract the content. You MUST return the extracted content   \"\n",
    "    \"as a single valid JSON object that adheres strictly to the provided schema. \"\n",
    "    \"Do not add any conversational filler.\"\n",
    ")\n",
    "\n",
    "def format_to_llama_instruct(doc):\n",
    "    \"\"\"\n",
    "    Converts a single structured document into the Llama 3.2 Instruct chat format.\n",
    "    The output content (the assistant's response) is formatted as a JSON string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # The prompt is the cleaned input text\n",
    "        user_prompt = (\n",
    "            f\"Extract the content for the specified keys from the following paper text:\\n\\n\"\n",
    "            f\"--- TEXT ---\\n{doc['input']}\"\n",
    "        )\n",
    "\n",
    "        # The expected output is the JSON object dumped as a string\n",
    "        assistant_response = json.dumps(doc['output'], ensure_ascii=False)\n",
    "\n",
    "        # Assemble the final Llama Instruct conversation structure\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_INSTRUCTION},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_response}\n",
    "            ]\n",
    "        }\n",
    "    except KeyError as e:\n",
    "        print(f\"Skipping document due to missing key: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during formatting: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_and_split_data():\n",
    "    \"\"\"\n",
    "    Loads data, formats it, shuffles it, and splits it into train/valid/test sets.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Error: Input file not found at '{INPUT_FILE}'. Please check the filename.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading data from {INPUT_FILE}...\")\n",
    "    formatted_data = []\n",
    "    \n",
    "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            try:\n",
    "                doc = json.loads(line)\n",
    "                \n",
    "                # IMPORTANT: Skip documents where the 'input' is empty (cleanup failure)\n",
    "                if not doc.get('input'):\n",
    "                    print(f\"Skipping line {line_num + 1}: Empty 'input' field.\")\n",
    "                    continue\n",
    "                \n",
    "                # Format the document for Llama 3.2\n",
    "                formatted = format_to_llama_instruct(doc)\n",
    "                if formatted:\n",
    "                    formatted_data.append(formatted)\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping line {line_num + 1}: JSON decode error: {e}\")\n",
    "            \n",
    "    total_count = len(formatted_data)\n",
    "    if total_count == 0:\n",
    "        print(\"No valid data found after loading and formatting. Exiting.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Total documents loaded and formatted: {total_count}\")\n",
    "    \n",
    "    # Shuffle the entire dataset to ensure random distribution of samples\n",
    "    random.seed(42) # Set seed for reproducibility\n",
    "    random.shuffle(formatted_data)\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_end = int(total_count * TRAIN_RATIO)\n",
    "    valid_end = train_end + int(total_count * VALID_RATIO)\n",
    "\n",
    "    # Split the data\n",
    "    train_set = formatted_data[:train_end]\n",
    "    valid_set = formatted_data[train_end:valid_end]\n",
    "    test_set = formatted_data[valid_end:]\n",
    "\n",
    "    print(f\"--- Dataset Split Summary ---\")\n",
    "    print(f\"Train set size: {len(train_set)}\")\n",
    "    print(f\"Validation set size: {len(valid_set)}\")\n",
    "    print(f\"Test set size: {len(test_set)}\")\n",
    "\n",
    "    # Save the split datasets\n",
    "    save_data(train_set, TRAIN_FILE)\n",
    "    save_data(valid_set, VALID_FILE)\n",
    "    save_data(test_set, TEST_FILE)\n",
    "    \n",
    "    print(\"\\nData splitting and formatting complete!\")\n",
    "    print(f\"Files saved: {TRAIN_FILE}, {VALID_FILE}, {TEST_FILE}\")\n",
    "\n",
    "\n",
    "def save_data(data, filename):\n",
    "    \"\"\"Saves the list of dictionaries to a JSONL file.\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for entry in data:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "            \n",
    "load_and_split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd202836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inspecting first 2 documents from: llama_train.jsonl ---\n",
      "\n",
      "================================================================================\n",
      "DOCUMENT EXAMPLE #1\n",
      "================================================================================\n",
      "ROLE: SYSTEM\n",
      "CONTENT: You are an expert academic document extractor. Your task is to process the full text of an academic ...\n",
      "\n",
      "ROLE: USER (Input Text)\n",
      "CONTENT: Extract the content for the specified keys from the following paper text:\n",
      "\n",
      "--- TEXT ---\n",
      "This paper presents the first detailed assessment of the invasive potential of Melaleuca hypericifolia Sm. in South Africa. This woody, fire-adapted shrub, native to Australia, is considered a high risk invader w... (Text truncated)\n",
      "\n",
      "ROLE: ASSISTANT (Target JSON)\n",
      "CONTENT:\n",
      "{\n",
      "  \"author_highlights\": [\n",
      "    {\n",
      "      \"endOffset\": 15253,\n",
      "      \"sentence\": \"The invasion risk of the Australian invader plant Melaleuca hypericifolia is assessed.\",\n",
      "      \"startOffset\": 15167\n",
      "    },\n",
      "    {\n",
      "      \"endOffset\": 15356,\n",
      "      \"sentence\": \"It poses a high risk of invasion with the potential to occupy 4% of South Africa if allowed to spread.\",\n",
      "      \"startOffset\": 15254\n",
      "    },\n",
      "    {\n",
      "      \"endOffset\": 15444,\n",
      "      \"sentence\": \"An integrated management approach involving mechanical and... (JSON truncated)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "DOCUMENT EXAMPLE #2\n",
      "================================================================================\n",
      "ROLE: SYSTEM\n",
      "CONTENT: You are an expert academic document extractor. Your task is to process the full text of an academic ...\n",
      "\n",
      "ROLE: USER (Input Text)\n",
      "CONTENT: Extract the content for the specified keys from the following paper text:\n",
      "\n",
      "--- TEXT ---\n",
      "Theoretical frameworks highlight the importance of threat-related information-processing biases for understanding the emergence of anxiety in childhood. The psychometric properties of several tasks measuring thes... (Text truncated)\n",
      "\n",
      "ROLE: ASSISTANT (Target JSON)\n",
      "CONTENT:\n",
      "{\n",
      "  \"author_highlights\": [\n",
      "    {\n",
      "      \"endOffset\": 13498,\n",
      "      \"sentence\": \"Psychometrics of a range of information-processing tasks were examined in children.\",\n",
      "      \"startOffset\": 13415\n",
      "    },\n",
      "    {\n",
      "      \"endOffset\": 13564,\n",
      "      \"sentence\": \"Bias scores showed poor reliability and convergence across tasks.\",\n",
      "      \"startOffset\": 13499\n",
      "    },\n",
      "    {\n",
      "      \"endOffset\": 13655,\n",
      "      \"sentence\": \"Implications for anxiety-related information-processing studies in children are discussed.\",\n",
      "     ... (JSON truncated)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def inspect_llama_file(filename, num_examples=2):\n",
    "    \"\"\"\n",
    "    Loads a Llama-formatted JSONL file and prints the first few examples \n",
    "    in a readable structure for verification.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Error: Output file not found at '{filename}'. Run the data preprocessing cell first.\")\n",
    "        return\n",
    "\n",
    "    print(f\"--- Inspecting first {num_examples} documents from: {filename} ---\")\n",
    "    \n",
    "    count = 0\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if count >= num_examples:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                example_doc = json.loads(line)\n",
    "                messages = example_doc.get(\"messages\", [])\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(f\"DOCUMENT EXAMPLE #{count + 1}\")\n",
    "                print(\"=\"*80)\n",
    "                \n",
    "                for message in messages:\n",
    "                    role = message.get(\"role\", \"N/A\").upper()\n",
    "                    content = message.get(\"content\", \"N/A\")\n",
    "                    \n",
    "                    if role == \"SYSTEM\":\n",
    "                        print(f\"ROLE: SYSTEM\\nCONTENT: {content[:100]}...\\n\")\n",
    "                    elif role == \"USER\":\n",
    "                        # Truncate user prompt (the full paper text) for clean display\n",
    "                        print(f\"ROLE: USER (Input Text)\\nCONTENT: {content[:300]}... (Text truncated)\\n\")\n",
    "                    elif role == \"ASSISTANT\":\n",
    "                        # Pretty print the target JSON output\n",
    "                        try:\n",
    "                            # The content is a JSON string, parse it to display cleanly\n",
    "                            parsed_json = json.loads(content)\n",
    "                            pretty_json = json.dumps(parsed_json, indent=2, ensure_ascii=False)\n",
    "                            print(f\"ROLE: ASSISTANT (Target JSON)\\nCONTENT:\\n{pretty_json[:500]}... (JSON truncated)\\n\")\n",
    "                        except json.JSONDecodeError:\n",
    "                            print(f\"ROLE: ASSISTANT (ERROR: Content is not valid JSON):\\n{content[:200]}...\\n\")\n",
    "                            \n",
    "                count += 1\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON line: {e}\")\n",
    "                break\n",
    "\n",
    "INSPECT_FILE = \"llama_train.jsonl\" \n",
    "\n",
    "inspect_llama_file(INSPECT_FILE, num_examples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab4df44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e78795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b36ed81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c04f162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "import evaluate\n",
    "import json\n",
    "from rouge_score import rouge_scorer\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01989fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\islam hitham\\Paper-LLM-Extractor\\notebooks\\structured_input_output_fixed_1.jsonl\"\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=file_path, split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcfc3e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Example 1 \n",
      "Input (first 300 chars): Objective The overall research objective was to theoretically and empirically develop the ideas around a system of safety management practices (ten practices were elaborated), to test their relationship with objective safety statistics (such as accident rates), and to explore how these practices wor\n",
      "Output keys: ['author_highlights', 'bib_entries', 'docId', 'metadata', 'abstract', 'body_text']\n",
      "\n",
      " Example 2 \n",
      "Input (first 300 chars): The objective of an accident-mapping algorithm is to snap traffic accidents onto the correct road segments. Assigning accidents onto the correct segments facilitate to robustly carry out some key analyses in accident research including the identification of accident hot-spots, network-level risk map\n",
      "Output keys: ['author_highlights', 'bib_entries', 'docId', 'metadata', 'abstract', 'body_text']\n",
      "\n",
      " Example 3 \n",
      "Input (first 300 chars): The Driver Behavior Questionnaire (DBQ) is a self-report measure of driving behavior that has been widely used over more than 20 years. Despite this wealth of evidence a number of questions remain, including understanding the correlation between its violations and errors sub-components, identifying \n",
      "Output keys: ['author_highlights', 'bib_entries', 'docId', 'metadata', 'abstract', 'body_text']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate safely, skip empty/malformed lines\n",
    "examples = []\n",
    "for i, item in enumerate(islice(dataset, 3)):\n",
    "    # Sometimes streaming returns string or dict\n",
    "    if isinstance(item, str):\n",
    "        item = item.strip()\n",
    "        if not item:  # skip blank lines\n",
    "            continue\n",
    "        try:\n",
    "            item = json.loads(item)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Skipping malformed line {i+1}\")\n",
    "            continue\n",
    "    elif not item:\n",
    "        continue\n",
    "\n",
    "    examples.append(item)\n",
    "    print(f\" Example {len(examples)} \")\n",
    "    print(\"Input (first 300 chars):\", item[\"input\"][:300])\n",
    "    print(\"Output keys:\", list(item[\"output\"].keys()))\n",
    "    print()\n",
    "\n",
    "if not examples:\n",
    "    print(\"No valid examples found. Check your JSONL file for empty or malformed lines.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e05a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = r\"C:\\Users\\islam hitham\\Paper-LLM-Extractor\\models\\llama-3.2-1b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de750a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c163ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    results = rouge.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    # Basic JSON structure accuracy\n",
    "    correct_json = 0\n",
    "    for p in pred_str:\n",
    "        try:\n",
    "            json.loads(p)\n",
    "            correct_json += 1\n",
    "        except:\n",
    "            pass\n",
    "    json_acc = correct_json / len(pred_str)\n",
    "    \n",
    "    return {\"rougeL\": results[\"rougeL\"], \"json_accuracy\": json_acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f3c642",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_llama3b_output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    warmup_steps=20,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./qlora_llama3b_adapter\")\n",
    "tokenizer.save_pretrained(\"./qlora_llama3b_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e66c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py330",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
